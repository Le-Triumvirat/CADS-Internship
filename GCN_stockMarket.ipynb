{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Le-Triumvirat/CADS-Internship/blob/main/Anomaly_detection_in_financial_services_using_knowledge_graphs_and_machine_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Knowledge graph</h1>\n",
    "<p>Headline files:</p>\n",
    "<ol>\n",
    "    CNN\n",
    "    <li>boeing.csv</li>\n",
    "    <li>google.csv</li>\n",
    "    <li>walmart.csv</li>\n",
    "    <br>Reuters\n",
    "    <li>apple.csv</li>\n",
    "    <li>microsoft.csv</li>\n",
    "    <li>samsung.csv</li>\n",
    "</ol>\n",
    "\n",
    "Headlines dataset are present in the directory \"headlines\"\n",
    "To install pandas, perform the following command in the terminal\n",
    "\n",
    "> **pip install pandas**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "directory = './headlines/'\n",
    "headline_filenames = ['boeing.csv','google.csv','walmart.csv','apple.csv','microsoft.csv','samsung.csv']\n",
    "stock_filenames = {'apple':'AAPL.csv','boeing':'BA.csv','google':'GOOG.csv','microsoft':'MSFT.csv','samsung':'SSNLF.csv','walmart':'WMT.csv'}\n",
    "companies = ['boeing','google','walmart','apple','microsoft','samsung']\n",
    "months = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\n",
    "# headline_filenames[0][:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date                                              Title\n",
      "0  24-Feb-17  Trump rally: CEOs of Dow companies make $400 m...\n",
      "1  24-Feb-17  Has my pilot had too much to drink? It depends...\n",
      "        Date                                              Title\n",
      "0  27-Feb-17  Historic Oscar victories for ESPN, Netflix and...\n",
      "1  25-Feb-17  Apple, Microsoft, PayPal join legal fight for ...\n",
      "        Date                                              Title\n",
      "0  27-Feb-17  Ellen DeGeneres and Walmart give these student...\n",
      "1  24-Feb-17  Trump rally: CEOs of Dow companies make $400 m...\n",
      "              Date                                              Title\n",
      "0  2011/10/18 0:00                 Apple iPhone 4S: Believe the hype?\n",
      "1  2011/10/18 0:00  Tech wrap: Apple misses, Intel beats quarterly...\n",
      "                 Date                                              Title\n",
      "0  2011-10-26 0:00:00  Tech wrap: RIM delays tablet update, Nokia unv...\n",
      "1  2011-11-01 0:00:00  Tech wrap: Yahoo finds interclick, pays $270 m...\n",
      "         Date                                         Title\n",
      "0  2011-10-20  Tech wrap: Basic phones lift Nokia to profit\n",
      "1  2011-10-28    Tech wrap: Google takes another shot at TV\n"
     ]
    }
   ],
   "source": [
    "for i in filenames:\n",
    "    a=pd.read_csv(directory+i)\n",
    "    print(a.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Apple and Microsoft don't have headers in their csv files, the following code includes them. Just execute it once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset of apple.csv\n",
      "   2011/10/17 0:00       Tech wrap: Apples Siri spurs iPhone 4S sales\n",
      "0  2011/10/18 0:00                 Apple iPhone 4S: Believe the hype?\n",
      "1  2011/10/18 0:00  Tech wrap: Apple misses, Intel beats quarterly...\n"
     ]
    }
   ],
   "source": [
    "print('Original dataset of apple.csv')\n",
    "apple=pd.read_csv(directory+'apple.csv')\n",
    "print(apple.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated dataset of apple.csv\n",
      "              Date                                              Title\n",
      "0  2011/10/18 0:00                 Apple iPhone 4S: Believe the hype?\n",
      "1  2011/10/18 0:00  Tech wrap: Apple misses, Intel beats quarterly...\n"
     ]
    }
   ],
   "source": [
    "print('Updated dataset of apple.csv')\n",
    "apple.to_csv(directory+'apple.csv',header=[\"Date\",\"Title\"],index=False)\n",
    "apple=pd.read_csv(directory+'apple.csv')\n",
    "print(apple.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset of microsoft.csv\n",
      "   2011-10-20 0:00:00       Tech wrap: Basic phones lift Nokia to profit\n",
      "0  2011-10-26 0:00:00  Tech wrap: RIM delays tablet update, Nokia unv...\n",
      "1  2011-11-01 0:00:00  Tech wrap: Yahoo finds interclick, pays $270 m...\n"
     ]
    }
   ],
   "source": [
    "print('Original dataset of microsoft.csv')\n",
    "microsoft=pd.read_csv(directory+'microsoft.csv')\n",
    "print(microsoft.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated dataset of microsoft.csv\n",
      "                 Date                                              Title\n",
      "0  2011-10-26 0:00:00  Tech wrap: RIM delays tablet update, Nokia unv...\n",
      "1  2011-11-01 0:00:00  Tech wrap: Yahoo finds interclick, pays $270 m...\n"
     ]
    }
   ],
   "source": [
    "print('Updated dataset of microsoft.csv')\n",
    "microsoft.to_csv(directory+'microsoft.csv',header=[\"Date\",\"Title\"],index=False)\n",
    "microsoft=pd.read_csv(directory+'microsoft.csv')\n",
    "print(microsoft.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Changing all the date category's format to yyyy/m/d and headlines to smallercase</h3>\n",
    "1. Google <br>\n",
    "2. Boeing <br>\n",
    "3. Walmart <br>\n",
    "4. Samsung <br>\n",
    "5. Microsoft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For Google, Boeing and Walmart\n",
    "'''\n",
    "def datePreProcess(company):\n",
    "    df=pd.read_csv('./headlines/'+company+'.csv')\n",
    "    df_len = len(df.index) # no of rows\n",
    "    if(company in ['google','boeing','walmart']):\n",
    "        for i in range(df_len):\n",
    "            temp = df['Date'][i].split('-')\n",
    "            temp[2] = '20'+temp[2]\n",
    "            df['Date'][i] = temp[2]+'/'+str(months.index(temp[1])+1)+'/'+temp[0]\n",
    "            df['Title'][i] = df['Title'][i].lower()\n",
    "        df.to_csv('./headlines/Pre-processed/'+company+'.csv',header=[\"Date\",\"Title\"],index=False)\n",
    "        \n",
    "    if(company=='samsung'):\n",
    "        for i in range(df_len):\n",
    "            temp = df['Date'][i].split('-')\n",
    "            if(temp[2][0]=='0'): # for day\n",
    "                temp[2]=temp[2][-1]\n",
    "            if(temp[1][0]=='0'): # for month\n",
    "                temp[1]=temp[1][-1]\n",
    "            df['Date'][i] = '/'.join(temp)\n",
    "            df['Title'][i] = df['Title'][i].lower()\n",
    "        df.to_csv('./headlines/Pre-processed/'+company+'.csv',header=[\"Date\",\"Title\"],index=False)\n",
    "        \n",
    "    if(company=='microsoft'):\n",
    "        for i in range(df_len):\n",
    "            temp = df['Date'][i][:-8].split('-')\n",
    "            if(temp[2][0]=='0'):\n",
    "                temp[2]=temp[2][-1]\n",
    "            if(temp[1][0]=='0'): # for month\n",
    "                temp[1]=temp[1][-1]\n",
    "            df['Date'][i] = '/'.join(temp)\n",
    "            df['Title'][i] = df['Title'][i].lower()\n",
    "        df.to_csv('./headlines/Pre-processed/'+company+'.csv',header=[\"Date\",\"Title\"],index=False)\n",
    "    \n",
    "    if(company=='apple'):\n",
    "        for i in range(df_len):\n",
    "            df['Date'][i] = df['Date'][i][:-5]\n",
    "            df['Title'][i] = df['Title'][i].lower()\n",
    "        df.to_csv('./headlines/Pre-processed/'+company+'.csv',header=[\"Date\",\"Title\"],index=False)\n",
    "    print(f'Successfully converted headlines to lowercase and updated the date format of {company}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted headlines to lowercase and updated the date format of google\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Google headline date preprocessing\n",
    "'''\n",
    "datePreProcess('google')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted headlines to lowercase and updated the date format of boeing\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Boeing headline date preprocessing\n",
    "'''\n",
    "datePreProcess('boeing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted headlines to lowercase and updated the date format of walmart\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Walmart headline date preprocessing\n",
    "'''\n",
    "datePreProcess('walmart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted headlines to lowercase and updated the date format of samsung\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Samsung headline date preprocessing\n",
    "'''\n",
    "datePreProcess('samsung')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted headlines to lowercase and updated the date format of microsoft\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Microsoft headline date preprocessing\n",
    "'''\n",
    "datePreProcess('microsoft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted headlines to lowercase and updated the date format of apple\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Apple headline date preprocessing\n",
    "'''\n",
    "datePreProcess('apple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KDJ (Min and Max) 0.0\t 1.0\n",
      "W%R (Min and Max) -1.0\t 0.0\n",
      "RSI (Min and Max) -92.080017\t 41.369995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "KDJ  |  W%R  |  RSI\n",
    "'''\n",
    "import pandas as pd\n",
    "goog = pd.read_csv('Stock Price\\GOOG.csv')\n",
    "print(f\"KDJ (Min and Max) {goog['KDJ'].min()}\\t {goog['KDJ'].max()}\")\n",
    "print(f\"W%R (Min and Max) {goog['W%R'].min()}\\t {goog['W%R'].max()}\")\n",
    "print(f\"RSI (Min and Max) {goog['RSI'].min()}\\t {goog['RSI'].max()}\")\n",
    "row_count = len(goog)\n",
    "'''\n",
    "Labels\n",
    "0 - overbought > 0.8 (sell)\n",
    "1 - oversold < 0.2 (buy)\n",
    "2 - nil trend [0.2,0.8]\n",
    "Note : Only AAPL has a column called as label\n",
    "'''\n",
    "# goog['KDJ'].sum()/len(goog['KDJ'].index)\n",
    "# goog['KDJ'].max()\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>KDJ | W%R</h4>\n",
    "If it is between 20 and 80 the market is neutral, if it is above 80 it is bullish/overbought and if it is below 20 it is bearish/oversold. If it is below 0 or above 100 it is very bearish or very bullish, but also very-oversold and very-overbought.\n",
    "\n",
    "Overbought --> overvalued that its intrinsic(bullish)<br>\n",
    "Oversold --> undervalued that its intrinsic(bearish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(goog['High'])\n",
    "plt.show()\n",
    "plt.plot(goog['Close'],color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "label=[]\n",
    "for i in range(row_count):\n",
    "    if(goog['KDJ'][i] < 0.2):\n",
    "        label.append(1)\n",
    "    elif(goog['KDJ'][i] > 0.8):\n",
    "        label.append(0)\n",
    "    else:\n",
    "        status = ((goog['Close'][i]-goog['Open'][i])/goog['Open'][i])*100\n",
    "        if(status < -0.66):\n",
    "            label.append(0)\n",
    "        elif(status > 0.66):\n",
    "            label.append(1)\n",
    "        else:\n",
    "            label.append(2)\n",
    "# print(label)\n",
    "plt.hist(label)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "950\n",
      "841\n",
      "916\n"
     ]
    }
   ],
   "source": [
    "print(label.count(0))\n",
    "print(label.count(1))\n",
    "print(label.count(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x<20...80<x\n",
    "$100\n",
    "(+$70)\n",
    "$30\n",
    "((closing-opening)/(opening))*100\n",
    "<-0.75 sell\n",
    ">0.75 buy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>KDJ</th>\n",
       "      <th>W%R</th>\n",
       "      <th>RSI</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006/5/25</td>\n",
       "      <td>188.315186</td>\n",
       "      <td>190.262527</td>\n",
       "      <td>184.952072</td>\n",
       "      <td>190.257553</td>\n",
       "      <td>16495700</td>\n",
       "      <td>0.999063</td>\n",
       "      <td>-0.000937</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2006/5/26</td>\n",
       "      <td>191.032516</td>\n",
       "      <td>191.693222</td>\n",
       "      <td>188.787125</td>\n",
       "      <td>189.442856</td>\n",
       "      <td>7381600</td>\n",
       "      <td>0.225640</td>\n",
       "      <td>-0.774360</td>\n",
       "      <td>1.814697</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2006/5/30</td>\n",
       "      <td>187.917770</td>\n",
       "      <td>189.268982</td>\n",
       "      <td>184.524841</td>\n",
       "      <td>184.768265</td>\n",
       "      <td>8688100</td>\n",
       "      <td>0.051310</td>\n",
       "      <td>-0.948690</td>\n",
       "      <td>5.674591</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2006/5/31</td>\n",
       "      <td>185.692245</td>\n",
       "      <td>187.902878</td>\n",
       "      <td>182.204926</td>\n",
       "      <td>184.708649</td>\n",
       "      <td>16066300</td>\n",
       "      <td>0.439408</td>\n",
       "      <td>-0.560592</td>\n",
       "      <td>1.059616</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2006/6/1</td>\n",
       "      <td>185.563095</td>\n",
       "      <td>190.257553</td>\n",
       "      <td>184.599365</td>\n",
       "      <td>190.073761</td>\n",
       "      <td>12637600</td>\n",
       "      <td>0.967518</td>\n",
       "      <td>-0.032482</td>\n",
       "      <td>-4.365112</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2702</th>\n",
       "      <td>2017/2/17</td>\n",
       "      <td>823.020020</td>\n",
       "      <td>828.070007</td>\n",
       "      <td>821.655029</td>\n",
       "      <td>828.070007</td>\n",
       "      <td>1611000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.910034</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2703</th>\n",
       "      <td>2017/2/21</td>\n",
       "      <td>828.659973</td>\n",
       "      <td>833.450012</td>\n",
       "      <td>828.349976</td>\n",
       "      <td>831.659973</td>\n",
       "      <td>1262300</td>\n",
       "      <td>0.649014</td>\n",
       "      <td>-0.350986</td>\n",
       "      <td>-2.589966</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2704</th>\n",
       "      <td>2017/2/22</td>\n",
       "      <td>828.659973</td>\n",
       "      <td>833.250000</td>\n",
       "      <td>828.640015</td>\n",
       "      <td>830.760010</td>\n",
       "      <td>982900</td>\n",
       "      <td>0.459870</td>\n",
       "      <td>-0.540130</td>\n",
       "      <td>1.899963</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2705</th>\n",
       "      <td>2017/2/23</td>\n",
       "      <td>830.119995</td>\n",
       "      <td>832.460022</td>\n",
       "      <td>822.880005</td>\n",
       "      <td>831.330017</td>\n",
       "      <td>1472800</td>\n",
       "      <td>0.882046</td>\n",
       "      <td>-0.117954</td>\n",
       "      <td>0.429993</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2706</th>\n",
       "      <td>2017/2/24</td>\n",
       "      <td>827.729980</td>\n",
       "      <td>829.000000</td>\n",
       "      <td>824.200012</td>\n",
       "      <td>828.640015</td>\n",
       "      <td>1392200</td>\n",
       "      <td>0.925003</td>\n",
       "      <td>-0.074997</td>\n",
       "      <td>3.690002</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2707 rows  10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date        Open        High         Low       Close    Volume  \\\n",
       "0     2006/5/25  188.315186  190.262527  184.952072  190.257553  16495700   \n",
       "1     2006/5/26  191.032516  191.693222  188.787125  189.442856   7381600   \n",
       "2     2006/5/30  187.917770  189.268982  184.524841  184.768265   8688100   \n",
       "3     2006/5/31  185.692245  187.902878  182.204926  184.708649  16066300   \n",
       "4      2006/6/1  185.563095  190.257553  184.599365  190.073761  12637600   \n",
       "...         ...         ...         ...         ...         ...       ...   \n",
       "2702  2017/2/17  823.020020  828.070007  821.655029  828.070007   1611000   \n",
       "2703  2017/2/21  828.659973  833.450012  828.349976  831.659973   1262300   \n",
       "2704  2017/2/22  828.659973  833.250000  828.640015  830.760010    982900   \n",
       "2705  2017/2/23  830.119995  832.460022  822.880005  831.330017   1472800   \n",
       "2706  2017/2/24  827.729980  829.000000  824.200012  828.640015   1392200   \n",
       "\n",
       "           KDJ       W%R       RSI  Label  \n",
       "0     0.999063 -0.000937  0.000000      0  \n",
       "1     0.225640 -0.774360  1.814697      0  \n",
       "2     0.051310 -0.948690  5.674591      1  \n",
       "3     0.439408 -0.560592  1.059616      2  \n",
       "4     0.967518 -0.032482 -4.365112      0  \n",
       "...        ...       ...       ...    ...  \n",
       "2702  1.000000  0.000000 -2.910034      0  \n",
       "2703  0.649014 -0.350986 -2.589966      2  \n",
       "2704  0.459870 -0.540130  1.899963      2  \n",
       "2705  0.882046 -0.117954  0.429993      0  \n",
       "2706  0.925003 -0.074997  3.690002      0  \n",
       "\n",
       "[2707 rows x 10 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goog['Label']=label\n",
    "goog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woah\n"
     ]
    }
   ],
   "source": [
    "goog.to_csv('goog_labeled.csv',index=False)\n",
    "print('woah')\n",
    "# df.to_csv('./headlines/Pre-processed/'+company+'.csv',header=[\"Date\",\"Title\"],index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5277"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "TF-IDF trial\n",
    "'''\n",
    "import pandas as pd\n",
    "# from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "googH = pd.read_csv('./headlines/Pre-processed/google.csv')\n",
    "type(googH['Title'])\n",
    "headlines_list=googH['Title'].values.tolist()\n",
    "len(headlines_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5772)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TF-IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>005</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>06</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>08</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zuck</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zuckerberg</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zucks</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zune</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zynga</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5772 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            TF-IDF\n",
       "000            0.0\n",
       "005            0.0\n",
       "06             0.0\n",
       "07             0.0\n",
       "08             0.0\n",
       "...            ...\n",
       "zuck           0.0\n",
       "zuckerberg     0.0\n",
       "zucks          0.0\n",
       "zune           0.0\n",
       "zynga          0.0\n",
       "\n",
       "[5772 rows x 1 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfIdfVectorizer=TfidfVectorizer(analyzer='word', use_idf=True)\n",
    "tfIdf = tfIdfVectorizer.fit_transform(headlines_list)\n",
    "print(tfIdf[0].shape)\n",
    "print(type(tfIdf))\n",
    "# print(tfIdf[0].todense())\n",
    "df = pd.DataFrame(tfIdf[0].T.todense(), index=tfIdfVectorizer.get_feature_names(), columns=[\"TF-IDF\"])\n",
    "# df = df.sort_values('TF-IDF', ascending=False)\n",
    "df\n",
    "# [0:{'word':'value'}]\n",
    "# df.to_csv('./TFIDF'+'.csv',header=[\"TF-IDF\"],index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 51)\t0.2854558802199217\n",
      "  (0, 33)\t0.2854558802199217\n",
      "  (0, 41)\t0.2854558802199217\n",
      "  (0, 15)\t0.2854558802199217\n",
      "  (0, 2)\t0.2854558802199217\n",
      "  (0, 11)\t0.2250566947475355\n",
      "  (0, 37)\t0.2854558802199217\n",
      "  (0, 48)\t0.18220279066644673\n",
      "  (0, 16)\t0.2854558802199217\n",
      "  (0, 31)\t0.2854558802199217\n",
      "  (0, 13)\t0.2854558802199217\n",
      "  (0, 10)\t0.2250566947475355\n",
      "  (0, 1)\t0.2250566947475355\n",
      "  (0, 47)\t0.2854558802199217\n",
      "['always', 'am', 'and', 'be', 'but', 'calculus', 'cease', 'choice', 'correction', 'destroy', 'done', 'exist', 'finite', 'half', 'hardest', 'hope', 'humanity', 'if', 'in', 'inevitable', 'is', 'it', 'its', 'killed', 'left', 'life', 'little', 'lives', 'me', 'nearly', 'needs', 'of', 'one', 'remember', 'requires', 'resources', 'simple', 'still', 'stones', 'strongest', 'the', 'they', 'this', 'to', 'unchecked', 'universe', 'used', 'when', 'will', 'wills', 'work', 'you']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'Hardest choice in the lives requires the strongest wills',\n",
    "    'When I am done, half of humanity will still exist. And I hope they remember you',\n",
    "    'Little one, its a simple calculus. This universe is finite, its resources finite. If life is left unchecked, life will cease to exist. It needs correction.',\n",
    "    'I used the stones to destroy the stones. It nearly killed me, but the work is done. It always will be. I am...inevitable'\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X[1])\n",
    "X\n",
    "print(vectorizer.get_feature_names())\n",
    "# print(len(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['calculus', 'cease', 'choice', 'correction', 'destroy', 'exist', 'finite', 'half', 'hardest', 'hope', 'humanity', 'inevitable', 'killed', 'left', 'life', 'little', 'lives', 'nearly', 'needs', 'remember', 'requires', 'resources', 'simple', 'stones', 'strongest', 'unchecked', 'universe', 'used', 'wills', 'work']\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "vectorizer1 = TfidfVectorizer(analyzer='word', stop_words='english', use_idf=True)\n",
    "X = vectorizer1.fit_transform(corpus)\n",
    "# print(X)\n",
    "print(vectorizer1.get_feature_names())\n",
    "print(len(vectorizer1.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['historic oscar victories for espn, netflix and amazon',\n",
       "  'apple, microsoft, paypal join legal fight for transgender rights',\n",
       "  \"why you shouldn't freak out (yet) about the 'cloudbleed' security leak\",\n",
       "  \"google's waymo sues uber over self-driving car technology\",\n",
       "  'tech criticizes trump on transgender reversal',\n",
       "  'google commits $11.5 million to racial justice efforts',\n",
       "  'lyft expands into 54 new cities, while uber does damage control',\n",
       "  'apple criticizes trump over reversal of transgender bathroom guidelines',\n",
       "  'these 10 stocks dominate the market',\n",
       "  \"russia accuses western media of spreading 'fake news'\"]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[headlines_list[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3)\t0.3779644730092272\n",
      "  (0, 1)\t0.3779644730092272\n",
      "  (0, 5)\t0.3779644730092272\n",
      "  (0, 4)\t0.3779644730092272\n",
      "  (0, 6)\t0.3779644730092272\n",
      "  (0, 2)\t0.3779644730092272\n",
      "  (0, 0)\t0.3779644730092272\n",
      "['apple', 'bathroom', 'criticizes', 'guidelines', 'reversal', 'transgender', 'trump']\n"
     ]
    }
   ],
   "source": [
    "vectorizer2 = TfidfVectorizer(analyzer='word', stop_words='english', use_idf=True)\n",
    "X = vectorizer2.fit_transform([headlines_list[7]])\n",
    "print(X)\n",
    "# print(X.shape)\n",
    "print(vectorizer2.get_feature_names())\n",
    "# print(len(vectorizer2.get_feature_names()))\n",
    "# X.shape # --> (documents,features). Features are unique words in a single document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>GCN implementation</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2165\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "542\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<generator object DataFrame.iterrows at 0x0000021F7F823E40>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "goog_lab = pd.read_csv('goog_labeled.csv')\n",
    "goog_lab, goog_lab_test = train_test_split(goog_lab, test_size=0.2, train_size=0.8, shuffle=False)\n",
    "print(len(goog_lab))\n",
    "print(type(goog_lab))\n",
    "print(len(goog_lab_test))\n",
    "goog_lab.iterrows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>KDJ</th>\n",
       "      <th>W%R</th>\n",
       "      <th>RSI</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006/5/25</td>\n",
       "      <td>188.315186</td>\n",
       "      <td>190.262527</td>\n",
       "      <td>184.952072</td>\n",
       "      <td>190.257553</td>\n",
       "      <td>16495700</td>\n",
       "      <td>0.999063</td>\n",
       "      <td>-0.000937</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2006/5/26</td>\n",
       "      <td>191.032516</td>\n",
       "      <td>191.693222</td>\n",
       "      <td>188.787125</td>\n",
       "      <td>189.442856</td>\n",
       "      <td>7381600</td>\n",
       "      <td>0.225640</td>\n",
       "      <td>-0.774360</td>\n",
       "      <td>1.814697</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2006/5/30</td>\n",
       "      <td>187.917770</td>\n",
       "      <td>189.268982</td>\n",
       "      <td>184.524841</td>\n",
       "      <td>184.768265</td>\n",
       "      <td>8688100</td>\n",
       "      <td>0.051310</td>\n",
       "      <td>-0.948690</td>\n",
       "      <td>5.674591</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2006/5/31</td>\n",
       "      <td>185.692245</td>\n",
       "      <td>187.902878</td>\n",
       "      <td>182.204926</td>\n",
       "      <td>184.708649</td>\n",
       "      <td>16066300</td>\n",
       "      <td>0.439408</td>\n",
       "      <td>-0.560592</td>\n",
       "      <td>1.059616</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2006/6/1</td>\n",
       "      <td>185.563095</td>\n",
       "      <td>190.257553</td>\n",
       "      <td>184.599365</td>\n",
       "      <td>190.073761</td>\n",
       "      <td>12637600</td>\n",
       "      <td>0.967518</td>\n",
       "      <td>-0.032482</td>\n",
       "      <td>-4.365112</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2160</th>\n",
       "      <td>2014/12/23</td>\n",
       "      <td>524.118103</td>\n",
       "      <td>531.636780</td>\n",
       "      <td>523.411987</td>\n",
       "      <td>527.688477</td>\n",
       "      <td>2203600</td>\n",
       "      <td>0.519951</td>\n",
       "      <td>-0.480049</td>\n",
       "      <td>-4.688721</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2161</th>\n",
       "      <td>2014/12/24</td>\n",
       "      <td>527.608948</td>\n",
       "      <td>528.851074</td>\n",
       "      <td>524.138000</td>\n",
       "      <td>525.878418</td>\n",
       "      <td>707800</td>\n",
       "      <td>0.369274</td>\n",
       "      <td>-0.630726</td>\n",
       "      <td>2.810059</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2162</th>\n",
       "      <td>2014/12/26</td>\n",
       "      <td>525.878418</td>\n",
       "      <td>531.328491</td>\n",
       "      <td>524.426392</td>\n",
       "      <td>531.109680</td>\n",
       "      <td>1043400</td>\n",
       "      <td>0.968298</td>\n",
       "      <td>-0.031702</td>\n",
       "      <td>-4.231262</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2163</th>\n",
       "      <td>2014/12/29</td>\n",
       "      <td>529.279724</td>\n",
       "      <td>532.551758</td>\n",
       "      <td>527.112671</td>\n",
       "      <td>527.429932</td>\n",
       "      <td>2284800</td>\n",
       "      <td>0.058330</td>\n",
       "      <td>-0.941670</td>\n",
       "      <td>4.679748</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2164</th>\n",
       "      <td>2014/12/30</td>\n",
       "      <td>525.202148</td>\n",
       "      <td>528.245422</td>\n",
       "      <td>524.247375</td>\n",
       "      <td>527.519409</td>\n",
       "      <td>878600</td>\n",
       "      <td>0.818408</td>\n",
       "      <td>-0.181592</td>\n",
       "      <td>0.910523</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2165 rows  10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date        Open        High         Low       Close    Volume  \\\n",
       "0      2006/5/25  188.315186  190.262527  184.952072  190.257553  16495700   \n",
       "1      2006/5/26  191.032516  191.693222  188.787125  189.442856   7381600   \n",
       "2      2006/5/30  187.917770  189.268982  184.524841  184.768265   8688100   \n",
       "3      2006/5/31  185.692245  187.902878  182.204926  184.708649  16066300   \n",
       "4       2006/6/1  185.563095  190.257553  184.599365  190.073761  12637600   \n",
       "...          ...         ...         ...         ...         ...       ...   \n",
       "2160  2014/12/23  524.118103  531.636780  523.411987  527.688477   2203600   \n",
       "2161  2014/12/24  527.608948  528.851074  524.138000  525.878418    707800   \n",
       "2162  2014/12/26  525.878418  531.328491  524.426392  531.109680   1043400   \n",
       "2163  2014/12/29  529.279724  532.551758  527.112671  527.429932   2284800   \n",
       "2164  2014/12/30  525.202148  528.245422  524.247375  527.519409    878600   \n",
       "\n",
       "           KDJ       W%R       RSI  Label  \n",
       "0     0.999063 -0.000937  0.000000      0  \n",
       "1     0.225640 -0.774360  1.814697      0  \n",
       "2     0.051310 -0.948690  5.674591      1  \n",
       "3     0.439408 -0.560592  1.059616      2  \n",
       "4     0.967518 -0.032482 -4.365112      0  \n",
       "...        ...       ...       ...    ...  \n",
       "2160  0.519951 -0.480049 -4.688721      1  \n",
       "2161  0.369274 -0.630726  2.810059      2  \n",
       "2162  0.968298 -0.031702 -4.231262      0  \n",
       "2163  0.058330 -0.941670  4.679748      1  \n",
       "2164  0.818408 -0.181592  0.910523      0  \n",
       "\n",
       "[2165 rows x 10 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goog_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006\n"
     ]
    }
   ],
   "source": [
    "for _ , row in goog_lab.iterrows():\n",
    "    print(row['Date'][:4])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014']\n"
     ]
    }
   ],
   "source": [
    "# [Jan,Feb,March]\n",
    "# (1,2,3)\n",
    "\n",
    "# [April,May,Jun]\n",
    "# (4,5,6)\n",
    "\n",
    "# [Jul,Aug,Sep]\n",
    "# (7,8,9)\n",
    "\n",
    "# [Oct,Nov,Dec]\n",
    "# (10,11,12)\n",
    "\n",
    "# Connect graphs based on these relationships [Quarter-wise]\n",
    "first_quarter = ('1','2','3')\n",
    "second_quarter = ('4','5','6')\n",
    "third_quarter = ('7','8','9')\n",
    "fourth_quarter = ('10','11','12')\n",
    "\n",
    "list_of_years = []\n",
    "num_of_rows = len(goog_lab.index)\n",
    "# pd.unique(goog_lab['Date'])\n",
    "# for i in range(num_of_rows):\n",
    "#     if goog_lab['Date'][i][:4] not in list_of_years:\n",
    "#         list_of_years.append(goog_lab['Date'][i][:4])\n",
    "#     else:\n",
    "#         continue\n",
    "for _ , row in goog_lab.iterrows():\n",
    "    if row['Date'][:4] not in list_of_years:\n",
    "        list_of_years.append(row['Date'][:4])\n",
    "    else:\n",
    "        continue\n",
    "print(list_of_years)\n",
    "# goog_lab['Date'][0][:4]=='2006'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "goog_graph = nx.Graph()\n",
    "\n",
    "goog_graph.add_nodes_from(['0','1','2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_of_rows-1):\n",
    "    if (goog_lab.iloc[i]['Date'][5:6] in first_quarter) and (goog_lab.iloc[i+1]['Date'][5:6] in first_quarter):\n",
    "        goog_graph.add_edges_from( [ ( goog_lab.iloc[i]['Date'] , goog_lab.iloc[i+1]['Date'] ) ] )\n",
    "    \n",
    "    elif (goog_lab.iloc[i]['Date'][5:6] in second_quarter) and (goog_lab.iloc[i+1]['Date'][5:6] in second_quarter):\n",
    "        goog_graph.add_edges_from( [ ( goog_lab.iloc[i]['Date'] , goog_lab.iloc[i+1]['Date'] ) ] )\n",
    "        \n",
    "    elif (goog_lab.iloc[i]['Date'][5:6] in third_quarter) and (goog_lab.iloc[i+1]['Date'][5:6] in third_quarter):\n",
    "        goog_graph.add_edges_from( [ ( goog_lab.iloc[i]['Date'] , goog_lab.iloc[i+1]['Date'] ) ] )\n",
    "        \n",
    "    elif (goog_lab.iloc[i]['Date'][5:6] in fourth_quarter) and (goog_lab.iloc[i+1]['Date'][5:6] in fourth_quarter):\n",
    "        goog_graph.add_edges_from( [ ( goog_lab.iloc[i]['Date'] , goog_lab.iloc[i+1]['Date'] ) ] )\n",
    "    \n",
    "    else:\n",
    "        continue\n",
    "    goog_graph.add_edges_from( [ ( goog_lab.iloc[i]['Date'] , str(goog_lab.iloc[i]['Label']) ) ] )\n",
    "\n",
    "goog_graph.add_edges_from( [ ( goog_lab.iloc[num_of_rows-1]['Date'] , str(goog_lab.iloc[num_of_rows-1]['Label']) ) ] )\n",
    "\n",
    "# goog_graph.add_edges_from([(1,2)])\n",
    "\n",
    "# pos = nx.kamada_kawai_layout(goog_graph)\n",
    "# nx.draw(goog_graph,pos,with_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2168\n"
     ]
    }
   ],
   "source": [
    "print(len(goog_graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb4AAAEuCAYAAADx63eqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAABvxElEQVR4nO3dd1hUx9fA8S+w9C5SREEUC4piiWLvIgYVY+8ae48xMRo1xkY0xkRj1FhijL1E1NiwR7FrsKCiCBaaAlJEOizsvn/wuj9XUEEXljKf5+GJsLP3Hm6Ac2fuzBkNuVwuRxAEQRDKCE11ByAIgiAIRUkkPkEQBKFMEYlPEARBKFNE4hMEQRDKFJH4BEEQhDJFJD5BEAShTBGJTxAEQShTROITBEEQyhSR+ARBEIQyRSQ+QRAEoUwRiU8QBEEoU0TiEwRBEMoUkfgEQRCEMkUkPkEQBKFMEYlPEARBKFNE4hMEQRDKFJH4BEEQhDJFJD5BEAShTBGJTxAEQShTROITBEEQyhSR+ARBEIQyRSQ+QRAEoUyRqDsAQRAEdYhNzsD7egSBUYkkpmdhoifBycaEPp9UwsJIV93hCYVIQy6Xy9UdhCAIQlHxD09g9dmH+AbFAJCRJVO8pifRRA60rWnJhDbVqGdnpp4ghUIlEp8gCGXGtish/OATSHpWNu/6y6ehAXoSLWZ7ODG4qUORxScUDTHUKQhCmZCT9O6TJpW9t61cDmnSbH7wuQ8gkl8pIya3CIJQbGRkZDBy5EgqV66MsbEx9evX5+jRo4rXT58+jZOTEwYGBrRr147Q0FCl944YMQITExNsbGxYtmyZ4jX/8AQWHrhFxOFVhK8YSNjyvkRtm5Hr/E/Xj0Ua/5SE89sJ/ak7D37sydDWtTAwNOLx48dKbXfu3MnAgQMJCgqie/fuWFpaUq5cOdzd3Xnw4IFS28ePH9O1a1eMjY0pX74806dPV9UlEz6ASHyCIBQbWVlZ2NnZ4evry8uXL/Hy8qJv376EhIQQGxtLz549WbhwIfHx8TRq1Ih+/fop3jtv3jyCg4MJDQ3lzJkz/PTTTxw7dgyA1Wcf8vTQCmTpSdiOXoPdlJ2YdxytdG7pi0iQydAuVxEAw1qtsP/am8rTvBm67ixVq1ZVan/kyBE8PDxISEjA09OTBw8eEB0djaurK927d1e0y8zMxM3Njfbt2xMVFUVERASDBw8urEso5IN4xicIQrHm4uLC3LlziYuLY9OmTVy6dAmAlJQUypcvz82bN3FycsLW1pZNmzbRqVMnAObMmUNwcDCrNmym8TebCd34JZUmbkZT1yDP8yT6HSTrRSTl3MaScH47WQmRlO82DQBdiSaXZrRXzPaUyWRUqFCBgIAAypcvr3Sc+Ph4LCwsiI2NxcLCgvXr17N161bOnz9fWJdIKCDR4xMEodiKjo4mKCgIZ2dnAgICqFevnuI1Q0NDHB0dCQgI4MWLF0RGRiq9Xq9ePQICAvC+HkH6syAkplYknN9O+IqBPPtzIimBF5XOlfbID33HRorPUx9eI/zX/jzbMIGE60fwvhGheO3atWtUrVo1V9IDOHfuHDY2NlhYWABw5coVHBwc+PTTTylfvjxt27blzp07KrtGQsGJxCcIQrEklUoZNGgQw4YNw8nJieTkZExNTZXamJqakpSURHJysuLzN18LjEokPSEGaUwomroGVJq0mXJu44g7shxpbDgAMmk6mZHB6Nm7AGBQqxW2o9ZQ6YvtlOs8mbhzO/jH+2/FsV8Nc74pIiKCiRMnKj1fjIiIYNeuXXzxxRc8e/aMLl260L17dzIzM1V3sYQCEYlPEIRiRyaTMWTIEHR0dFi1ahUARkZGJCYmKrVLTEzE2NgYIyMjxedvvpaYnoWGRAc0JZi26I+GljZ69nXRs69L2pMbAKSH+KNb0QkNiTYAOuXtkRhboKGphV6lWhg38uT+pZOKY/v4+ORKfDExMXTq1IkJEyYwYMAAxdf19fVp2bIln376KTo6OkybNo24uDju37+vwismFIRYziAoEdUsBHWTy+WMHDmS6OhofHx80NbOSUbOzs5s3rxZ0S4lJYVHjx7h7OyMubk5FSpUwN/fHzc3NwD8/f1xcHAg5UUM2lYOuU+koaH4Z9pjP/QdG789KA0NJJo57aOiooiMjKRhw4aKl1+8eEGnTp3w9PRk9uzZSm91cXHh4kXlYVVBvUTiE4D3VbOIYvmpIFHNQigS48eP5/79+5w6dQp9fX3F13v06ME333zD3r176dKlCwsWLMDFxQUnJycAhg4dyoIFC4iPj+fw4cPs3LkTIyMjmlZrg2kVF+JNLHl5+W9Mm/Ul49kD0sPuYN5uOABpj65j2rSP4lypQVfQta+Dpq4hmZFBJF8/RPdJM5DL5ezfv59WrVrx4sULypUrR2JiIu7u7rRo0YIff/wx1/czePBgfvnlF06dOkW7du347bffKF++PLVq1SrkKym8jZjVKYhqFkKxERoaioODA7q6ukgk/7svX7duHYMGDeLUqVNMmjSJ0NBQmjRpwl9//UVKSgrHjx/Hx8eHc+fOIZfL0dPTY/To0fzyyy/Ep0ppseRfkiKfEHf0N6QxIUhMrDBrPQSDms3JjAkh9sBP2I76XXG+mAM/kf7kJvJsKVrG5TFp0Jmkq3vJSkkAQENDg7Zt23L69Gk2b97M559/joGBARqv9SLv3buHvb09APv27WP69Ok8f/6chg0bsnr1apydnYvmogq5iMRXxhWkmsUr+tqazPaoJZKfoBbx8fGcOnWK48ePc/z4cbS1tencuTPu7u60b98eExOTXO8Zs9WPk/ej87yxe3nFG1laIubtRuR5Pg0NaGKrxz/TupCRkQGAjo4Oa9asYcSIvN8jFG9icksJUBKrWcTGxtKiRQssLCwwMzOjWbNmSs857t69i7u7O+XLl1e6SxaEN2VnZ3PlyhXmzZtHs2bNcHBwYPPmzdSvX59///2Xx48fs2bNGj777LM8kx7AxLbV0JNo5fmaxNQao7pubz2/nkSLWZ99wsmTJ9HT0wNyJt/MmDGDmTNnEhYW9vHfpFCkROIrAUpiNQsjIyM2btxITEwML168YMaMGXTr1o2srCwAtLW16du3L3/++WdhXjqhhHr69CkbN26kb9++WFlZMWbMGFJSUvDy8iImJoYjR44wefJkatSoka8bp3p2Zsz2cEJfO/efPMNardAub5fn+3JGN5xwqWRGq1atWLt2LQDTpk3j4sWLpKamUr9+fXr16sXZs2cRA2glgxjqLKFKUjULmUzGkSNH8PT0JDo6GisrK8VrDx8+pHr16uIPRhmXnp7O+fPnFcOXz549w83NDXd3dzp16kTFihVVch7F82xpNu/6iXvX8+y///6bjh07Uq5cOQCSkpLYsmULq1atQiKRMGnSJAYPHoyhoaFKYhZUT/T4SqDiXM2icuXKij8IkJOg9fT08PT0ZNSoUUpJTyi75HI5Dx484LfffsPDwwMrKyvmzp2LiYkJGzZs4Pnz5+zatYvhw4erLOlBzi4Lu8c0RTc2EIlGzv57r9OTaKIr0cS9tjW7xzTN8zl23759lX7GjY2NmThxIvfu3WP58uX4+PhQuXJlvv7661yPAoTiQSxnKGHyqmZhaWmp1Kag1SwMajSn0qTNZDwN5Pme+eiUt0e7vF2e1SyM6ndGy9CMjGdBxO5fxD/VKjKo4WQOHDjAzJkziYiI4Ny5c7Rt2xaA27dvk56ezv79+0WlijLu5cuX/Pvvvxw/fpxjx46RnZ2Nu7s7I0aMYPv27ZibmxdJHCZZCcT98yN3gkM4cDuKwMgkEtOlmOhp41TBmN4NP2zNqoaGBh07dqRjx448efKE33//HVdXV5o2bcrkyZNxc3NDU1P0NYoDkfhKkI+pZvHqofxbq1loailVs9Aub5dnNYtXXlWzuHBoJ2a/z0EikZCRkYGuri4GBsrDpnp6egwYMIBatWpRv359pR6oUHrJZDJu3LihGL68efMmzZs3x93dnS+++IJatWqpZWLTzp076dOnDzZmhoxt7Vgo56hSpQpLly5l/vz57Nixg+nTp5Oens6kSZMYNmzYWyfhCEVDJL4SQhXVLJ49e8aOHTtIT0/ntt9VlVSz0NXRJkNTUzHNOyMjg9atW2NsbIyDg4PSx8uXLzl7NmdCjLGxsUqui1C8REdHc+LECY4dO8bJkyexsLDA3d2dWbNm0bp161w3RUVNLpezfft2xSSVwmZgYMCoUaMYOXIk58+fZ+XKlcydO5dBgwYxadIkatasWSRxCMpEv7uEeFXN4tChQ7mqWdy9e5e9e/eSnp6uqGZRs2ZNQkJCaNiwIUOHDqVq1arUrl2bI0eO0K5dO7q1aoBpFRck/1/NQi7LJj3iHulhd9CvmlOKKe3RdeXne0FXyE5PRi6Xk/HsAcnXD9G1V38ePnyIo6MjWlpaGBoakpaWxubNmxk1ahSfffYZpqambN26lefPn7Nq1Sqsra2xsLCgYcOGjB8/HoBly5axb98+7ty5k6sHKxRfmZmZnD17lpkzZ9KgQQOcnJw4cOAAbdu2xc/Pj/v37/Prr7/SuXNntSc9yBl6T05Opnnz5kV6Xg0NDVq3bs2ePXvw9/fHxMSE1q1b4+7uzqFDh8jOzi7SeMo6MauzBMhvNYuQkBAqV65MrVq1uHHjBlKplBYtWvD06VPu3LmDoaEhM2bM4KuvviI2OeOjq1mYN+rC/b0rsDDSpXfv3lhYWJCZmclff/2Fr68vX3zxBY8fP0ZbW5u6deuycOFCWrdujVwu5/r16zRurNyb1NfXp0qVKoSEhKCrq6vUW6xcubLS529W6ReKzuPHjzl27BjHjx/n7Nmz1KxZE3d3dzp37kyTJk2UfkaLmxkzZqClpcWiRYvUHQrp6en8/fffrFy5kvj4eCZMmMCIESOK7FlnWSYSXwklk8m4c+cO586dU3zo6+vTpk0bWrduTevWralWrdo7n6F8bDUL99rWrB3ciKysLGxsbHj8+LFKnl3I5XLi4uIICQlR+ggNDVX8WyKR5EqGrydIMzOzUrswvqgLiScnJ3P27FlFsktOTsbd3R13d3fc3Nzy3JOuOJLJZDg4OODj40OdOnXUHY6CXC7n6tWrrFy5Eh8fH/r06cPkyZOpW7euukMrtUTiKyGkUik3b95UJLkLFy5gZWWlSHKtWrWicuXKBTqmf3gC/f+4Qpo09zBLyv3z6Fg6vGNhrxa7xzTFpZIZz58/Z+/evYphy8Iml8uJj4/PlQxf/9DQ0Hhrb9HBwQFzc/MSlxjfXUhcEzmopJC4XC7n9u3bitmX//33H66uropk5+LiUuKuHeRsEDt58mT8/f3VHcpbRUVFsX79etauXUuNGjWYPHky3bt3L9a96JJIJL5iKj09nf/++0+R6C5fvkyVKlUUia5169ZYW1t/9HlKY61OuVxOQkJCrt7ikydPFIkyOzv7rb1FBwcHLCwsitUfd1UUEpdKpfj4+ODp6Znre4uNjeXkyZMcP36cEydOYGhoqBi+bNu2rWKGcEk2duxYqlatyowZucvyFTeZmZns27ePlStXEh4ezvjx4xk9enSJ6V0XdyLxFRMpKSlcvnwZX19fzp07x/Xr16ldu7YiybVs2VJp0awqlcXdGRISEnL1Fl//PDMz8629xcqVK2NpaVlkiVEVNycpKSl06dIFX19fgoKCqFKlClevXlUMXz548IC2bdsqenWOjoUzzV9dMjMzsbW15caNG4odE0qKGzdusHLlSvbv389nn33G5MmT+eSTT9QdVokmEp+aJCQkcPHiRUWiu3v3Lg0aNKB169a0adOGZs2aFemU/9sRCfx+9iFnHsSgAaTnMYzWrqYlE9pWw6WSWZHFpS4vX74kNDQ0z6HU0NBQ0tLSlJJixYoVOXXqFIGBgSQlJVGtWjUWL17Mp59+CuQUEp84cSJhYWE0adKETZs2KYamMzIyGD9+PN7e3hgYGDB9+nS++uorIGd4s+/vZ3l2/A9SAy8gl2WhY1kFm8FLlOJ9un4sVr2/JyXgLC8v/42GVs5yFz1tLc6f8+Xzzz8nODgYmUyGjY0NUVFRODk5ER8fT1JSEhoaGtSqVYuff/6ZFi1aKI67fPlylixZQmpqKr1792bNmjXo6pa8DYkPHjzIzz//zLlz59QdygeLjY1lw4YN/P7771SqVInJkyfTq1cvdHR01B1aiSMSXxF5/vw558+fVwxdPnz4kCZNmigmo7i6uiotU1CXuOQMvG9EEBiZxJmLV3CoaM2nzVw+uJpFaZWUlKSUFIODgzl58iQSiYRnz56RmJhIVlYWrVq1wsHBgd27dzN27Fh69OjB7t27uXnzJleuXAFg5syZXLhwgYMHDxIVFUW7du3YtGkTnTt3ZsxWP3Ys+Qa5LJtybuPQ1DMi8/kTdG2qKWKRvojk+e7vqTjuD6V6qhoa0MLekIPTPUlKSlK0NzY2ZtGiRYwaNYrQ0FCqV6+OhoYGBw4cYMSIETx//hyJRMLx48cZOnQo//77L7a2tvTo0YOmTZvmudlqcdevXz/at2/P2LFj1R3KR8vKyuLgwYOsWrWKwMBAxo4dy9ixY7GxsVF3aCWGSHyF5FXprlcfz549o2XLloqhy4YNGxb7O7UpU6bg4ODA1KlT1R1KiZOSkkLDhg3p3bs3T548wdfXl5YtWxISEsKTJ0+IiYnBwcGB6tWrc/HiRfr27YubmxsODg78/fffREZGsvrPLR9dSFxbE/RO/EDc0xBiYmLQ1tYmLS2NqKgopWfEeRUSHzhwIA4ODoqp/6dPn2bQoEFERUUV/gVUoaSkJCpVqsTjx4+xsLBQdzgqdffuXVatWsXu3bv59NNPmTx5Mk2bNi1Wz6eLIzFVSAXkcjmPHz9WSnSJiYmKJDd27FhcXFzQ0sp7P7DiqkKFCkRGRqo7jBIpOTmZ0NBQhgwZwpo1a/D09GTNmjWK12vXrs3YsWOxtbXl5MmTGBkZcfDgQUJCQnjw4AEvX75E7tRRqZB4SsAZtIzMMW0xEEOn/w1Hpj3yw6Rxd8XnrwqJaxmVw7xxN6b9spWxrR1JT09n586d/Pjjj0pJz8XFhcDAQKRSqVIh8YCAALp3/99x69WrR3R0NHFxcSUqgezfv582bdqUqJjzq06dOqxdu5bFixezadMmBg8ejLm5OZMnT6Zfv36KUoWCMpH4PoBcLuf+/ftKiU4ulyuGLb/55hu11SFUpVdbDAkFk59C4ubm5piamtK0aVMAli5dqvgjdfLkSUaNGoV1zYakn72gkkLiY1vPRE9Pj8ePHzN48GClWN5WSDw5OTlXgXPI6UGVpCSyffv2Ur9Turm5OVOnTmXKlCkcPXqUlStX8s033zBq1CjGjx+PnV3ey5LKqhKT+Ip60e7rsrOzuX37tmIiyvnz5xUlh9zc3Fi4cCFVq1Yt8YnuTaLHV3CqKiRuYmJCilSmkkLi9y+dBGYC4OPjw/r163PFnVch8TfjfvXvklRnNTo6mqtXr7J//351h1IkNDU16dKlC126dOHBgwf8/vvv1KtXj/bt2zN58mRat25d6v5OfYhin/jevWg3iuWnglSyaPd1UqkUPz8/RW/u4sWL2Nra0rp1a3r37s1vv/1GpUqVVHKu4szGxkYkvgJQRSFxAH9/f5ydnTHRk6ikkLhEM6d9VFQUkZGRNGzY8K3NpVIpjx8/pl69ejg7O+Pv70/fvn0Vcb2qs1pS7N69G09Pz2JRJ7So1axZkxUrVuDl5cWWLVsYN24c2traTJ48mUGDBn3QNVFnB0SVinWR6m1XQuj/xxVO3o8mI0umlPQgZ8p9RpaME/ei6f/HFbZdCcnzOGlpae88T1paGmfPnmXBggWKnZUnTJjAs2fPGDlyJEFBQdy7d4+1a9cycODAMpH0QPT4CqqghcSdnJwAGDp0KF5eXrx48YLAwED++OMPPv/8c5xsTFRSSLxlx84AHD16lM6dOyvu+K9cucKFCxfIzMwkLS2NJUuWEB0dTZMmTRRx/fnnn9y7d4+EhAS8vLz4/PPPi+JSqsyOHTsYNGiQusNQq9c3yv3ll184fPgw9vb2TJs2jSdPnuTrGP7hCYzZ6keLJf+y/FQQ/9x6xr+Bz/nn1jN+PRVE8yX/MnabH/7hCYX7zahIsZ3V+eaiXXmWlLgTv5MecgtZejISMxvM2wxT+qXPq6LIli1bGDNmDHfv3qVatZwp4ElJSVy6dEnRo7t58yZ16tRRPKNr0aIFZmZmRfntFksymQw9PT2SkpJK5NqtopTfQuKhoaGKdXwODg6A8jo+fX39Qi0k3r9/f3r37g3wzkLiryxbtowlS5aQlpZGr169WLt2bYn5WXj48KGiSLso+aXs1Ua5f/31F82bN2fy5Ml07Ngxz2HQ0ljgolgmvrxqSMoy00m8uhejuh3RMrUk7ZEfsQeXYjtiFRKz/81Qe1VDso6tCTNnzmTVqlVkZ2czatQo9PT08PX15f79+3zyySeKRNe0adNSUZKpMNjZ2XHhwoUC1wEVVKO4FhIvCRYsWEBsbCy//fabukMptlJTU9m+fTsrV65EKpUyadIkhg4dqniOWxpLGkIxHepcffYh6VnKhZM1dfQwazUIiZk1GhqaGFRzRWJqTUbUQ6V26VnZ/HbqAY0bN2bZsmWkpqaSkZHBnj17MDc3Z9myZcTGxuLr66sY2hRJ7+3EcKd6TWxbDT1J3stgJKbWGNV1e+t79SRaTGibM8oRHx/PwoULy0zSk8vl7Nixg4EDB6o7lCKRkZHByJEjqVy5MsbGxtSvX5+jR48qXj99+jROTk4YGBjQrl07QkNDgZyNcocOHconn3xCWFgYX3/9NdbW1nzxxRccvnyHH3wCSUlNJe7474SvGEjY8r5Ebctd6/Tp+rFI45+ScH47gT90Y2jrWhgYGmFkZMTjx4+V2u7cuZOBAwcSFBRE9+7dsbS0pFy5cri7u/PgwQNFu02bNqGlpYWRkZHi4+zZsyq5XsUu8cUmZ+AbFPPOLjVAdsoLpPFP0bFUrrsnl4NvcCyBIU+Ry+UYGhqioaGBTCZj9uzZtGrVSqxtKQCR+NSrnp0Zsz2c0NfO/atqWKvVO3bP0GS2h5OivJyVlVWR7Z5RHNy4cYOsrCzF88rSLisrCzs7O3x9fXn58iVeXl707duXkJAQYmNj6dmzJwsXLiQ+Pp5GjRrRr18/xXvnzZvHw4cPiYiI4ObNmxgZGRETE8O8vy+TnpVN/LFVyNKTsB29BrspOzHvOFrp3NIXkSCToV2uIpDzc1l5mjdD150lOTmZqlWrKrU/cuQIHh4eJCQk4OnpyYMHD4iOjsbV1VVp3ShAs2bNSE5OVny0bdtWJder2A18e1+PeG8beXYWsQd/xqhuB7Qtcv/ia2lqsmz/RXrVNuXy5cucPXuW4OBg5HK5mMpbQGJmp/q9GjLKz3MWkKOvLSkRz1kK06veXln5fTc0NGTevHmKz7t27UqVKlW4fv06cXFxODs706dPHyAn0ZUvX57AwECcnJzYvHkzmzZtwtzcHHNzc8aOHcvd+w9IMrIjMzqU1OCrSpWDXi+XB5D26D+luRaQ0wE58yCGuOQMpdmeMpmMkydP8uuvv1K+fHlcXV0Vr02dOhUvL68iKZBQ7Hp8gVGJuWZvvk4ulxF7+BfQklDObVyebdKzZARGJlG+fHm6devGL7/8wsGDB8vML4EqVahQocSVqCqNBjd1YPeYprjXtkZXoomeRPlXV0+iiY6WBllP/Fjfr3g/Xyls2dnZiuG0sio6OpqgoCCcnZ0JCAigXr16itcMDQ1xdHQkICCAFy9eEBkZqfR6vXr1uHYjZ8/CjNcqB4WvGMizPyeSEnhR6Vxpj/yUZxb/f+WgkLXjmPy9cjH1a9euUbVq1Ty3Vzp37hw2NjZKSe/mzZuUL1+eGjVqsHDhQrKysj7uwvy/YtfjS0x/+zcml8uJ8/mN7JQErPrMQ0Pr7eEnpksLI7wyp0KFCvj5+ak7DAFwqWTG2sGNlAqJJ6ZLMdHTxqmCMb0bVmLO9MMc3LSKVkuXqjtctTl79iwVKlRQLBcpa/JTOcjU1JSkpCSSk5MVn7/5mnaWjOykuI+qHHTgryXsbFydAQMGAP8b5nxTREQEEydOZNmyZYqvtW7dmrt371K5cmUCAgLo168fEomEmTNnfvQ1KnaJz0Tv7SHFH1+NNC4c6/5eaGq/e0q1iZ62qkMrk8QzvuLHwkiXsa3z3i9vzpw51KlThylTppSZ9aZvKkuTWt6kqspBWno5w5ofWzmoUsd+eHt7KxJfXpWDYmJi6NSpExMmTFC0A5SeDdatW5fvv/+epUuXqiTxFbuhTicbE3QlucPKevmc5FvHyIx+TMTKIYT90puwX3qTHHAmV1s9iSZOFUpOWaXiTCS+kqVChQqMGTOG+fPnqzsUtXhVc7R///7qDqXIvV45aO/evUqVg/z9/RXt3lY56BV/f3+s7HJurD62cpCethavVszlVTnoxYsXdOrUCU9PT2bPnv3O709DQwNVrb4rdomv9yd536VKTK2o/O1hKn+zH/uvvRUfRs7tcrWVA70bls27XVUTk1tKnunTp/PPP/8QGBio7lCKnI+PDw0aNKBixYrqDqXIqbJyUKfP+uU8S7ar88GVg+TRwdw9uVsxU/PNykGJiYm4u7vTokWLPPd4PHr0KNHR0QAEBgaycOHCXLM+P1SxS3zljXRpU8OSD52HoqGRs1N4SaobV5xZW1sTExNDdnb2+xsLxYK5uTnTpk1jzpw56g6lyG3fvr1MDnOGhoaybt06bt26hY2NjWLd2/bt27G0tGTv3r3Mnj0bc3Nzrl69yq5duxTvnT9/Po6OjlSuXJk2bdrwzTffMHdCTpk3DS0Jlr2+I+2RH+HL+xJ/dCXlu0xF28KOzJgQNHX0kJhaKY6Vcv8cz9aOJnxZHyIP/sKM6dMZNmwYkPv53v79+/nvv//466+/lNbqhYWFATlrD11cXDA0NMTDw4OePXsya9YslVyvElO5Jb9eVW55tX5J+HiWlpbcvXtXaQ83oXhLTU2levXqHDhwgEaNGr3/DaVAQkIClStXJjQ0VJQc/EhZWVl0mLeb0GxTIO9eyLsqB71eNejV8YpT5aBi1+ODdy/afZc3F+0KqiGe85U8BgYGzJkzR2V3yCXBvn376NChg0h6H+nBgwe0aNEC2Z2jec63eOVdlYNerxoExa9yULFMfJCzbmm2Ry30tbXeO+ypoZHT0yvu9eFKKpH4SqaRI0fy+PFjTp8+re5QisT27dvL/E4MH0Mmk7FixQpatGjBsGHD8N2/hTldar+1A/K2ykF5dUCKW+WgYrec4XWDmzrgUsmM388+5MyDGDTIWZz+ip5EEzk5z/QmtK0menqFRExwKZm0tbXx8vJi5syZXL16tVQXcHj69Ck3b96kS5cu6g6lRAoNDWX48OGkp6dz+fJlqlevDhSsalBJ2p2hWCc+yN+iXTGRpXCJ6i0lV9++fVmyZAn79++nZ8+e6g6n0OzevZvPPvtM1OEtILlczqZNm5g+fTrTpk1j2rRpaGkpF0UvjR2QYjm5RSheVqxYwcOHD1m5cqW6QxE+wLFjx/jqq6+4fft2qd2X7pNPPuGnn36iQ4cO6g6lxIiKimLMmDGEhYWxdetW6tat+973lJYOSOn8LRBUqkKFCpw/f17dYQgfyN3dHUtLS7Zu3crw4cPVHY7KBQYGEhkZqbLK/WWBt7c3kyZNYtSoUXh7e6Ojo5Ov972ralBJIhKf8F5ickvJpqGhweLFi+nfvz8DBgwodcOBO3bsoH///rmG6ITc4uPjmTx5Mn5+fhw4cKDMbNv0pmI7q1MoPsTklpKvefPmNGjQgDVr1qg7FJWSy+ViNmc+HTt2DBcXFywsLLh582aZTXognvEJ+ZCcnIyVlRUpKSmlemZgaXf37l06dOhAcHBwsVlP9bGuXr3K0KFDCQwMFD+bb5GcnMy0adM4evQoGzduFM9BET0+IR+MjIzQ0tLKVeFdKFnq1KlD586dlbZ+Kele9fZE0svb+fPnqVevHhkZGdy+fVskvf8nenxCvtSoUYODBw+W2T3OSouQkBA++eQT7t+/j5WV1fvfUIxlZWVRsWJFLl68SLVq1d7/hjIkPT2dOXPmsH37dtauXYunp6e6QypWRI9PyBcxwaV0cHBwYNCgQSxatEjdoXy006dP4+DgIJLeG65fv84nn3zCkydP8Pf3F0kvDyLxCfkiJriUHrNnz2br1q2EhISoO5SPIia1KJNKpSxYsIBPP/2UWbNmsWfPnlw7rws5ROIT8kVUbyk9rK2tmThxIvPmzVN3KB8sNTWVQ4cO0bdvX3WHUizcv3+f5s2bc+nSJW7evCmee76HSHxCvoihztLl66+/5ujRowQEBKg7lA9y6NAhXF1dsbGxUXcoaiWTyVi2bBmtWrVi5MiRHD16tExuwltQYgG7kC8VKlTgzp076g5DUBFTU1OmT5/Od999x/79+9UdToHt2LGjTG44+7onT57w+eefk52dzdWrV3F0LPkVVYqK6PEJ+SJ6fKXPhAkT8PPz48qVK+oOpUDi4+Px9fWlR48e6g5FLeRyOX/88Qeurq507doVX19fkfQKSPT4hHwRk1tKH319febNm8e3337LmTNnSswzoT179uDu7l5qFuEXRGRkJKNGjSIyMpIzZ85Qp04ddYdUIoken5AvYnJL6TRs2DCioqI4ceKEukPJt7I6zLlr1y7q16/PJ598wpUrV0TS+whiAbuQL3K5HD09PV6+fFnqihyXdXv37mXRokX8999/aGoW73vhsLAwGjRoQGRkZL53FCjp4uLimDBhArdv32bz5s24urqqO6QSr3j/lAvFhoaGBtbW1qLXVwr17NkTTU1NvL291R3Ke+3atYtevXqVmaR35MgRXFxcsLW15caNGyLpqYhIfEK+iQkupdOrbYu+++47pFKpusN5p7KyaD0xMZFRo0YxceJEtm/fzvLly9HX11d3WKWGSHxCvtnY2IgeXynVsWNH7O3t+euvv9QdylvdvXuX+Ph4WrVqpe5QCtXZs2epV68eALdv3xYb7BYCkfiEfBM9vtJt8eLFLFiwgLS0NHWHkqcdO3YwYMCAYv8c8kOlpaUxdepUBg4cyMqVK9mwYUOZnLlaFErnT5BQKETiK90aN25M06ZNWbVqlbpDyUUmk7Fjx45SO8zp5+dHw4YNefbsGXfu3KFr167qDqlUE4lPyDeR+Eq/hQsXsnTpUhISEtQdipLLly9jaGiIi4uLukNRKalUyty5c+nSpQtz585l9+7dWFhYqDusUk8kPiHfROIr/WrVqkW3bt1YunSpukNRUho3nA0ICKBp06b8999/3Lx5k/79+6s7pDJDJD4h38TklrJh7ty5rF27ttjc5EilUvbs2cOAAQPUHYpKZGdn8/PPP9O2bVvGjRvHkSNHsLW1VXdYZYooWSbkm+jxlQ329vZ8/vnneHl5sXr1anWHw4kTJ6hRowZVqlRRdygf7fHjxwwbNgxNTU2uXr1K1apV1R1SmSR6fEK+WVtbExMTQ3Z2trpDEQrZzJkz2b17N48fP1Z3KKVi7Z5cLmfdunW4urrSo0cPzpw5I5KeGomSZUKBWFlZcfv27TK/D1pZsGDBAoKCgti2bZvaYkhOTqZSpUoEBweX2N3Enz59yqhRo3j+/Dlbt26ldu3a6g6pzBM9PqFARLHqsmPq1KmcOnUKf39/tcVw4MABWrRoUSKTnlwuZ8eOHTRo0IAmTZpw5coVkfSKCfGMTyiQV9sT1a9fX92hCIXM2NiYWbNmMXv2bA4fPqyWGErqMGdsbCzjx48nICAAHx8fGjVqpO6QhNeIHp9QIGKCS9kyduxY7t69y4ULF4r83DExMVy6dInu3bsX+bk/xqFDh3BxccHe3p7r16+LpFcMiR6fUCAi8ZUturq6LFiwgG+//Zbz588X6Tq6v//+my5dumBkZFRk5/wYiYmJfPnll5w9e5Zdu3bRunVrdYckvIXo8QkFIp7xlT2DBg0iISEBHx+fIj3v9u3bS8yGs//++y8uLi5IJBL8/f1F0ivmROITCkT0+MoeLS0tfvjhB2bOnIlMJiuScz5+/Jjg4GA6depUJOf7UKmpqUyZMoUhQ4bw+++/s379eoyNjdUdlvAeIvEJBfJqcotQtnh6emJoaMjOnTuL5Hw7d+6kT58+aGtrF8n5PsTVq1dp0KABz58/586dO3h4eKg7JCGfROITCkT0+MomDQ0NfvzxR+bMmUNmZmahnksulxfr2ZyZmZl89913eHp6snDhQnbu3Em5cuXUHZZQACLxCQXyKvGJugdlT5s2bahZsyYbNmwo1PP4+/uTlpZG8+bNC/U8H+LOnTs0adKEW7ducevWLfr27avukIQPIBKfUCCGhoZoa2uTmJio7lAENVi0aBFeXl6kpKQU2jm2b9/OgAEDitVODNnZ2SxZsoT27dszadIkDh06RIUKFdQdlvCBROITCkwMd5ZdDRo0oHXr1qxYsaJQji+Tydi5c2exGuZ8+PAhrVu35ujRo/z333+MHDmyWCVloeBE4hMKTExwKdsWLlzIsmXLiIuLU/mxz507h4WFBc7Ozio/dkHJ5XJ+//13mjZtSp8+ffj3339xcHBQd1iCCogF7EKBiR5f2Va9enV69+7NkiVL+Omnn1R67B07dhSL3l5ERAQjRowgISGB8+fPU6tWLXWHJKiQ6PEJBSYWsQvff/89f/75J0+fPlXZMTMyMti7d69adyKXy+Vs3bqVhg0b0qpVKy5duiSSXikktiUS8i0gIIBZs2Zx/fp1Xr58iUwmY8mSJUyaNEndoQlqMGPGDF68eMH69etVcrx//vmH5cuX4+vrq5LjFVRMTAzjxo3jwYMHbNmyhYYNG6olDqHwiR6fkG+ampocPnyYp0+fkpycjEwmw9XVVd1hCWoyY8YM9u/fT1BQkEqOp85hzgMHDuDi4oKjoyN+fn4i6ZVyoscnFMg333zDypUrycjIwN7enpCQEDHDrQxbvHgxt27dYvfu3R91nMTEROzs7Hjy5EmRLgZ/+fIlU6ZM4fz582zevJmWLVsW2bkF9RE9PqFAFixYoKhFOGnSJJH0yrgvvviC8+fPc+PGjY86zr59+2jbtm2RJr3Tp0/j4uKCvr4+/v7+IumVISLxCQWir6+veKbz+eefqzcYQe0MDQ2ZM2cOs2bN+qjj7Nixo8h2YkhNTWXy5MkMGzaMdevWsWbNmhKz9ZGgGmKoUyiQ2OQMvK9H4PfwGTKJHiZ6EpxsTOjzSSUsjHTVHZ6gBpmZmdSqVYsNGzbQrl27Ar8/KiqKWrVq8fTpUwwMDAohwv+5cuUKQ4cOxdXVlZUrV2Jubl6o5xOKJ5H4hHzxD09g9dmH+AbFAJCR9b/tafQkmsiBtjUtmdCmGvXszNQTpKA2O3bs4LfffuPy5csFHv5esWIFN27cYPPmzYUUXc5Sifnz57Nx40ZWrVpF7969C+1cQvEnhjqF99p2JYT+f1zh5P1oMrJkSkkPIP3/v3biXjT9/7jCtish6glUUJv+/fuTlpbGgQMHCvzewt5w9vbt27i6unL37l1u3bolkp4gEp/wbtuuhPCDz33SpNm8b2xALoc0aTY/+NwXya+M0dTUZPHixcyePZvs7Ox8vy84OJjQ0FA6dOig8piysrJYvHgxHTp0YOrUqRw4cAAbGxuVn0coeUTiE97KPzyBH3wCSZMWbNftNKmMH3wCuR2RUDiBCcXSp59+Srly5di2bVu+37Njxw769euHRKLa6onBwcG0atWKkydP4ufnx+effy5mIAsKIvGVUhkZGYwcOZLKlStjbGxM/fr1OXr0qOL106dP4+TkhIGBAe3atSM0NFTpvSNGjKBxjUoELRtA4rX9SseWSdOJO/474SsGEra8L1HbZuQ6/8PVo1i08zQAN27coHXr1hgZGWFtbZ2rsv/ly5cVe6/NmTOHunXrIpFImDdvnqouh1AENDQ0WLx4MXPnziUjI+O97Qtjw1mZTMaqVato1qwZAwcO5NSpU1SuXFllxxdKB5H4SqmsrCzs7Ozw9fXl5cuXeHl50bdvX0JCQoiNjaVnz54sXLiQ+Ph4GjVqRL9+/RTvnTdvHvcCH2A/cSM2Axbz8upe0h5fV7wef2wVsvQkbEevwW7KTsw7jlY6t/RFJMhk3Ew0ICjkKZ07d2bs2LHExcXx8OFDOnXqpNT+yJEjeHh4AFCtWjV++uknunTpUohXRygsLVu2pG7duqxdu/a9ba9fv67S6j/h4eG4u7uzdetWLl68yOTJk9HUFH/ihNzErM4yxMXFhblz5xIXF8emTZu4dOkSACkpKZQvX56bN2/i5OSEra0tvb9axPGX1mRkyUg4txXpi2dYdp+BNC6cyM1fUWniZjR18556nuh3kKwXkdh+Op4qTw5SXiOZrVu3vjWuhg0bsmHDBqUyUYMHD6ZatWqi11cC3b59m06dOhEcHKwodpCXqVOnYmxszIIFCz7qfK8KS0+bNo0pU6YwY8YMlQ+dCqWLuB0qI6KjowkKCsLZ2ZmAgADq1auneM3Q0BBHR0cCAgJ48eIFkZGRpBlXUsze1LaqgjQ2DICMZ0FITK1IOL+d8BUDefbnRFICLyqdK+2RH/qOjUjPknH31nXKlStH8+bNsbKyolu3boSFhSnaRkZGEh0dTYMGDYrgKghFwcXFBTc3N5YtW/bWNtnZ2ezateujZ3M+f/6cnj17snTpUk6cOMHs2bNF0hPeSyS+MkAqlTJo0CCGDRuGk5MTycnJmJqaKrUxNTUlKSmJ5ORkADI09RSvaeoaIstIAyA7KQ5pTCiaugZUmrSZcm7jiDuyHGlsOJDz/C8zMhg9excAEmOj2Lx5MytWrCAsLIwqVaowYMAAxbF9fHzo3LmzmHhQysyfP5/ffvuNmJiYPF8/c+YMtra2ODk5ffA59u/fT7169ahZsyZ+fn7Ur1//g48llC3i1qiUk8lkDBkyBB0dHVatWgWAkZERiYmJSu0SExMxNjZWlG7Sk/9vcoIsIxVNXX0ANCQ6oCnBtEV/NDS10LOvi559XdKe3EC7vB3pIf7oVnRCQ6INQGpaOprpaYwePRp7e3ssLS25dOkSf/zxBzVr1mTPnj0MHz68KC6FUISqVq3KgAEDWLx4cZ49v4+Z1JKQkMAXX3zB5cuX2bt3r2JilCDkl0h8pZhcLmfkyJFER0fj4+ODtnZOMnJ2dlaqkpGSksKjR49wdnbG3NycChUqoJ8Uga7EiowsGdLnT9Aubw+AtpVD7hO91ltLe+yHvmNjIKeiS8Mmzalqoc/XX39NeHg4gYGBABw9epSNGzdy7do1Tp8+zdSpU7Gzs1N83L9/n8zMTK5evYqdnR02NjZiokIJ89133+Hs7MyXX36Jvb09mZmZVKlShSZNmnDixAnmz59f4GOePHmSkSNH0q1bN27duoWhoWEhRC6UduIvSSk2fvx47t+/z6FDh9DX11d8vUePHty9e5e9e/eSnp7OggULcHFxUQw7DR06lP/2byA7LQlpXDjJ/scxqtsRAD27OkhMLHl5+W/ksmzSI+6RHnYH/ao5E1PSHl1H37ERAHLg2ynjOHbsGBKJhK5du/L8+XNatmzJvn37WLRoEW3atCEtLY2rV6+yfPlyevTogZWVFcnJyfj7+zN+/Hjq16+Pvr4+Dg4OtGrVikGDBvHtt9+yevVqDh48yM2bN4mNjUXM0ypebGxsGD9+vGKCkra2NjExMezfv5/09HTq1q2b701sU1JSmDhxIiNHjmTDhg2sXr1aJD3hg4keXykVGhrKunXr0NXVVapWsW7dOgYNGsTevXuZNGkSgwcPpkmTJuzatUvRZv78+YwfPx6/1SPI1tTGtGkv9Kt+AoCGlgTLXt8Rd/Q3Eq94IzGxonyXqWhb2JEZE4Kmjh4SUys0NKBdTUt6dGlE1KJFdOnShdTUVFq2bMmOHTuA/y1jkEgkip7e+vXrc9Vs/Ouvv+jfvz9Pnz4lPDyc8PBwwsLCuHPnDj4+PoqvZWRkKPUaX33Y29sr/v2uWYaC6n3zzTdUr16de/fuUbt2baytrYmIiCA7OxupVJqvNXaXLl1i2LBhNGvWjNu3b2NmZlb4gQulmljOILyVf3gC/f+4Qpo0fyWoXl7xRpaWiHm7Eehra7F7TFNcKpm9tX3t2rXx9vamdu3aKok3OTlZkRRfJcM3P7S1tfNMiK8+KlWqhJ6e3vtPJuTb0qVLuXz5Mvv27aNp06ZcvXoVAwMDDhw4QMeOHd/6voyMDObOncvmzZtZvXo1PXv2LMKohdJM9PiEt6pnZ8ZsD6f/r9X5/rJlElNrdKo1QV9bk9keTu9MepmZmQwdOlRlSQ9yJu3UqlWLWrVq5fm6XC4nPj4+VzI8ceKEImE+e/YMU1PTPJPiq2RZoUIFMWW+ACZNmsSKFSu4du0a2traaGpqcuzYMVq1aqXU7s8//8TExIQ+ffpw69Ythg4diqOjI/7+/lhZWakpeqE0Ej0+4b1yClUHkp717kLVGhqgJ9FitocTg5s6FFl8qiSTyYiOjs6VHF/vRcbExGBtbf3W4VQ7OzssLS3L/GScV3s3BkYlcjfoEZFhT3BrVBtXSxkDe3ZTavv8+XOqVKkC5Dyb3rJlCz///DNDhgwRS10ElROJT8iX2xEJ/H72IWcexKBBzlZEr2hrgkwux622DRPaVntnT680kEqlPHv2LM+k+OojKSmJihUrvrXnaGdnh5mZWan8o/6uvRu1NXN2cnhz78YRI0awbds2pFIpxsbG3LlzR9TYFAqNSHxCgcQlZ+B9I4LAyCQS06WY6GmjnxnPgeUzCbhxVd3hFRupqalERES89VljWFgYcrn8rUnxVcIs7B3JVe1DRgcaGKfQoEEDpFIpABKJhJ9++ompU6cWUdRCWSMSn/DRZDIZlSpV4syZM9SsWVPd4ZQYL1++fGtSDA8PJyIiAgMDg3cOqVasWBEdHR11fyvA63s35n8bK31tTeQ39/Hg8AZ0dXWpVKkS1apVY8SIEfTt27cQoxXKMpH4BJWYOHEidnZ2fPvtt+oOpdSQy+XExsa+c0g1MjISCwuLt85SfbX4X0tLq1BjfdsM4Oy0JOJ8VpAechNNfRPM2wzD0LmtUhtdiQbr+tSiTV2HUjn0KxQ/IvEJKnH69GlmzpzJtWvX1B1KmZKdnU1UVNQ7l3DExcVha2v7zmFVCwuL9yadwMBAxo0bx8qVK6lbt67Sa2O2+nHyfnSu4c2YAz+BXI6FxxdkRj/mufd8bAYvRcfyf8/vNDTAvbY1awc3Utl1EYR3EYlPUImsrCxsbGy4ceMG9vb26g5HeE1GRobS4v+8epDp6elUqlTpncOqhw8fZvjw4WhpaTF16lTmzp2Ljo4OsckZtFjyr9IkFgBZZjrhv/bHdtRqtMtVBCD20C9oGVtg3vZzpba6Ek0uzWiPhZFuUV0WoQwTiU9QmREjRlCvXj2mTJmi7lCEAkpJSXlrUnz1IZVKyczMBEBLSwtjY2P27NnDQ+0qLD8VlCvxZUY9ImrbdOyn7VV87eXVfWSE3cGqz1yltnoSTaa61WBsa8fC/2aFMk+swhVU5tW+aCLxlTyGhoY4OTm9dZsguVzOsGHD2Lp1K1paWmhqaqKpqcnTp08J1LPIlfQAZNI0NHT1lb6mqWuALDMtV9v0LBmBkUmq+WYE4T3K9gpbQaU6duyIv78/0dHR6g5FUDENDQ1SU1OxsLBg6tSp+Pv7ExcXx7Bhw0hMz8rzPZra+sgzlJOcPCMVTR39PNsnpktVHrcg5EX0+ASV0dPTo3Pnzhw4cIAxY8aoOxxBxXbs2IFEIslVkcZEL+8/I5JyFZHLspHGP1U848t8/gRty7wXppvoaas2YEF4C9HjE1SqV69e7Nu3T91hCIVAR0cnzzJsTjYm6Epyf11TRw+Dms1IOL8dWWY66RH3SH14FUPndrna6kk0caogds4QioaY3CKoVHJyMra2toSFhYntY8qIt83qhPyt4wPQ0dLg8rcdxKxOoUiIxCeoXPfu3enTpw+DBw9WdyiCis2YMYP//vsPqVRKamoqT58+ZciQIbx06ZfnOr78kWORGsH1FeNUHa4g5EkMdQoq17NnTzHcWUqFhITg6+vLhQsXuHHjBs+fP8fDw4OJbauhJ/mw6jB6Ei2envyLO3fuqDhaQcibSHyCynXr1o3Tp0+TkpKi7lAEFcrMzMTZ2RmZLGdIU19fn2XLltGuXTvF3o362gX7k6Kvrcl3XWrx7ZgBzJ49uzDCFoRcROITVK5cuXI0adKEY8eOqTsUQQWys7PZunUrTk5OXLp0iR49eqClpYWrq6vSms3BTR2Y7VELfW0t3ldyU0MD9LW1mO1Ri8FNHRg3bhz+/v5cvHixkL8bQRDP+IRCsnbtWs6fP8/27dvVHYrwgeRyOQcPHmT27NmYmJiwePFi2rRpQ3R0NH369MHb2zvPndHftXejnkQTOdCupmWuvRv/+usvNm7cyLlz50SxaqFQicQnFIrIyEhq165NVFQUurpipl5Jc+bMGWbOnElaWho//PADXbp0KXAyenPvxrTEeM4f2sXFrb9gb10uV/usrCxcXFz4+eef8fDwUNW3Igi5yQWhkLRo0ULu4+Oj7jCEAvjvv//kbm5uckdHR/n27dvl2dnZKjv2r7/+KgfkzZs3l2dmZubZZt++ffJ69eqp9LyC8CbxjE8oNL169WLv3r3vbyio3f379+nduzfdu3enZ8+e3L9/n4EDB+a5YP1DXb58GQA/Pz8GDx6MPI/Bps8++wxdXV12796tsvMKwptE4hMKTY8ePThw4ABZWXnXchTULzQ0lBEjRtCmTRtcXV0JDg5m3LhxaGurvnzYq70aMzMz2bt3L/Pnz8/VRkNDgx9//JE5c+YodoIQBFUTiU8oNA4ODtjb23PhwgV1hyK84fnz50yZMoWGDRtia2tLUFAQ06dPx8DAoFDOl56eTmhoKPr6+mhoaNC+fXuaNGmSZ9t27dpRtWpV/vzzz0KJRRBE4hMKlVjMXry8fPmSOXPmUKtWLeRyOffu3cPLy6vQy8vp6Ojw66+/cvLkSSwsLPjjjz/49NNP39p+0aJFLFy4UKwFFQqFSHxCoXpVtPrVomdBPdLS0li6dCnVq1cnPDyc69ev89tvv2FtbV0k59fU1GTy5Mm0aNGCTp06cfLkyXe2b9SoES1btuS3334rkviEskUkPqFQOTk5YWJiwn///afuUMokqVTKunXrqF69OpcvX+bs2bNs2rQJBwcHtcXk5ubGiRMn3ttu4cKFLFu2jBcvXhRBVEJZIhKfUOjEcGfRk8lk7Ny5k9q1a+Pt7c2+ffvYt28ftWvXVndouLm5cfr0abKzs9/ZrmbNmnz22WcsWbKkiCITygqxgF0odDdu3KBfv34EBQWJihyFTC6X4+Pjw+zZs9HV1WXRokV06NBB3WHl4uzszKZNm2jcuPE720VERODi4sKdO3eoWLFiEUUnlHaixycUugYNGpCVlcXdu3fVHUqpdv78eVq1asX06dOZO3cuV65cKZZJD8jXcz6ASpUqMXLkSBYuXIhcLufy5ct5rv8ThIIQiU8odBoaGmK4sxDdvHkTDw8Phg4dypgxY7h9+zY9evQo1r3r/D7nA/j222/ZsWMH1apVo3nz5gQHBxdydEJpJxKfUCR69uwpqrioWFBQEP3798fDwwMPDw8CAwMZOnQoWlofti9eUWrTpg1+fn4kJye/s11ERARdunQhPT2dx48fY2xsTEZGRhFFKZRWIvEJRaJZs2bExMSIu3UViIiIYPTo0TRv3hwXFxcePnzIpEmTSlQxcENDQxo3boyvr+87271KeK96r3K5XFR0ET6aSHxCkdDU1OSzzz5j//796g6lxIqNjeXrr7+mXr16WFhYEBQUxKxZszA0NFR3aB/Ezc3tvc/5qlWrRnBwMB4eHujo6JCamioSn/DRROITisyrxexCwSQlJTF//nxq1qxJWload+/e5ccff6Rcudxb+5QknTp1ytdzPlNTU/bv38/atWuRy+VERUURm5zBWt9HfLn7JiM2/8eXu2+y1vcRccliGFR4P7GcQSgyUqkUGxsb/P39qVSpkrrDKfbS09NZs2YNP/74I25ubsyfPx9HR0d1h6Uy2dnZWFtbc+vWrXz/PJwPCGPrjef4BsUAkJHHJrdta1oyoU016tmZFULUQmkgenxCkdHW1qZbt25iuPM9srKy+PPPP6lRowZnz57l1KlTbNu2rVQlPQAtLS06dOiQr2UNANuuhDBm9z1O3o8mI0umlPQgZ6f3jCwZJ+5F0/+PK2y7ElIIUQulgUh8QpESyxreTiaTsWfPHurUqcPWrVvZvXs3Bw4coG7duuoOrdDk5zkf5CS9H3zukybN5n1jVHI5pEmz+cHnvkh+Qp7EUKdQpNLS0qhQoQLBwcFYWlqqO5xiQS6Xc+LECWbNmgXk7EzQqVOnYr0OT1VCQ0Np3LgxUVFRb9301j88gf5/XCFNqlziLPH6IVLunCYzJgTDWm0o33Vqrvfqa2uxe0xTXCqZFUb4QgklenxCkdLX18fd3Z2DBw+qO5Ri4fLly7Rr144pU6Ywc+ZM/Pz8cHd3LxNJD6By5cqYm5vj7+//1jarzz4kPSt3XU+JkQWmzfth5OL21vemZ2Xz+9mHKolVKD1E4hOKnFjMDnfu3MHT05N+/foxdOhQ7t69S+/evctMwnvdu8qXxSZn4BsUk+fwpkHN5hjUaIamvslbjy2Xw5kHMWK2p6BEJD6hyHl4eHDhwgVevnyp7lCK3KNHjxg8eDBubm60b9+eoKAgRowYgUQiUXdoavOu8mXe1yM++vgagPeNjz+OUHqIxCcUOWNjY9q0acORI0fUHUqRefbsGePHj6dJkybUqFGD4OBgvvzyS/T09NQdmtq1bduWq1evkpqamuu1wKjEXLM3Cyo9S0ZgZNJHHUMoXUTiE9SirMzujI+PZ8aMGdSpUwdDQ0MCAwP5/vvvMTY2VndoxYaJiQn169fn/PnzuV5LTM9SyTkS06UqOY5QOojEJ6iFp6cnJ0+ezPMuvzRITk7mhx9+oEaNGrx48YLbt2/z888/U758eXWHViy97TmfiZ5qhoBN9LRVchyhdBCJT1ALCwsLGjduzPHjx9UdikplZGSwcuVKqlevzp07d7h06RLr168XlWre423P+ZxsTNCV5P1nSi7LRp6VCbJskMuQZ2Uil+We/akn0cSpguhhC/8jEp+gNqVpuDM7O5vNmzfj5OTE0aNH8fHxYdeuXdSoUUPdoZUIjRo1Ijw8nMjISKWv9/7k7TcMLy/uIuznniRe8SYl4AxhP/fk5cVdudrJgd4NxY2H8D9iAbugNs+ePaNOnTpERUWho6Oj7nA+iFwu559//uG7777D3NycxYsX06pVK3WHVSL16tWLzz77jCFDhih9fcxWP07ej35vxZa8aGiAe21r1g5upKIohdJA9PgEtbG1tcXJyYkzZ86oO5QPcvr0aZo2bcr8+fNZunQp58+fF0nvI7ytfNnEttXQk3zY5rp6Ei3GtyldNU6FjycSn6BWJXGromvXrtGxY0fGjRvH1KlTuXHjBh4eHmVy8bkqvZrg8uYgVD07M2Z7OKGvXbA/V/rampg+Pk2jqla0bduWn376iYsXL5Kenq7KsIUSSAx1Cmr1+PFjmjVrxrNnz9DS+rC7+qJy7949vvvuO65du8b333/P8OHD0dYWswVVydHRkX/++UepMLe/vz9jx46laqeh3KQq6VnvLlStoZHT05vt4UR1jWiaNWuGXC5X/L/q3bs3O3bsKOxvRSjGRI9PUKuqVatia2vLxYsX1R3KW4WEhPD555/Ttm1bmjdvTnBwMGPGjBFJrxC86vVlZWXh7e1NgwYNcHV15erVq7SpmFNw2r22NboSTfTemO2pJ9FEV6KJe21rdo9pyuCmDjRp0oRPPvkEyNkPUltbGy8vL3V8a0IxUnbrJAnFxqvZna1bt1Z3KEqio6P54Ycf2L59OxMnTiQ4OBhTU1N1h1Wqubm5sX79etLS0pgzZ45i2FNbW5sePXpgZWXG2sGNiEvOwPtGBIGRSSSmSzHR08apgjG9G1bCwkhX6Zjz5s2jT58+ZGdno6Ojw/Hjxxk/frw6vj2hmBBDnYLaBQQE8OmnnxIaGlosnpMlJCTw888/s2bNGoYMGcKsWbOwsrJSd1hlQkJCAvb29oSGhtK3b19Onz6NXC6nUqVKhIeHf9AxZTIZzs7OjBs3jq5du+Lp6Unr1q1ZsWJFiZ1NLHwcMdQpqF3t2rXR19fHz89PrXGkpqby008/UaNGDZ49e8aNGzf49ddfRdIrQmZmZjg7O3PhwgWioqJo27YtGhoadOzY8YOPqampSUBAAFOmTMHR0ZHLly/z9OlT3NzciImJUWH0QkkhEp+gdhoaGmqd3SmVSlmzZg3Vq1fn2rVr+Pr6snHjRipXrqyWeMq6jh078uWXX9K8eXNOnz7N3r17+eqrrz7qmK9vcmtiYsI///xDq1atcHV1fedegELpJIY6hWLh9OnTDBw4kNq1a/PixQtu3bpV6OeUyWTs3LmT77//HkdHRxYtWkSjRmKhs7r16dOH48ePExcXV+gTiHbt2sXkyZNZu3YtvXr1KtRzCcWHmNwiqN3nn3/Ozp07kUqlnD17lurVqxfq+eRyOYcPH2b27NkYGBiwYcMG2rVrV6jnFPJn9erV3LlzBw0NDRISErC0tCzU8/Xv358aNWrQo0cP7ty5w/fff6/UOxRKJ/F/WFC7Dh06oKWlpZjB5+zsXGjn8vX1pWXLlsyaNQsvLy8uX74skl4xcezYMby8vPDx8aFt27acOnWqSM7bsGFDrl27xsmTJ+nTpw/JyclFcl5BfUTiE9RuyJAh/Pbbb+jq5kxDb9CggcrPcePGDTp37szw4cMZP348t27dwtPTs1jMIhXg7t27DB06FG9vb6pWrfrW8mWFxdramn///RczMzNatGhBSEhIkZ1bKHoi8QnFwqhRo/jll1+AnC2LVOXBgwf07dtXMY09MDCQwYMHF/sqMWVJVFQUXbt25ddff6VFixZAzkL2EydO5CpfVph0dXXZsGEDI0eOpFmzZvj6+hbZuYWiJSa3CMXKL7/8QufP+nA+QkpgVCKJ6VmY6ElwsjGhzye5Fye/TXh4OPPnz+fAgQN8/fXXTJ48GUNDw0KOXiiotLQ02rZti4eHB3PnzlV8XS6X4+DgwLFjx6hVq1aRx3Xq1CkGDRrE/PnzGTduXJGfXyhcIvEJxYZ/eAKrzz7ENyhnbVVGlkzxmp5EEznQtqYlE9pUo56dWZ7HiImJYfHixWzevJmxY8fyzTffYG5uXgTRCwUlk8no378/2trabNu2Ldew8+jRo6lTpw5TpkxRS3wPHz7E09OTtm3bsmLFClGirhQRQ51CsbDtSgj9/7jCyfvRZGTJlJIeQPr/f+3EvWj6/3GFbVdClF5PTExk3rx5ODk5kZmZSUBAAIsWLRJJrxj7/vvvefr0KX/++Weez1qL+jnfm6pVq8aVK1cIDw8Xi91LGZH4BLXbdiWEH3zukyZ9d9V9ALkc0qTZ/OBzn21XQkhPT2fZsmVUr16dx48f4+fnx6pVq7CxsSma4IUPsnnzZnbu3Mk///yDnp5enm06dOjAuXPnyMzMLOLo/ufVYvfmzZvj6urK7du31RaLoDpiqFNQK//wBPr/cYU0aXaB36utISf9yGI+qWKJl5cXderUKYQIBVXz9fWlT58++Pr6vvf5XePGjfn5559p06ZNEUX3djt37uSLL75g3bp19OzZU93hCB9B9PiED5KRkcHIkSOpXLkyxsbG1K9fn6NHjypeP336NE5OThgYGNCuXTtCQ0OV3jtixAhMTExoVteR6EveSseWSdOJO/474SsGEra8L1HbZuQ6/9P1Y0mNjaDpyLl8//33TJgwASMjI6ytrVmxYoVS28uXL9O8eXOeP3/OgAEDsLW1xdTUlBYtWnD16lUVXxnhXYKDg+nXrx87duzI16SVV9sUFQcDBgzg6NGjfPnll8yfPx+ZTPb+NwnFkkh8wgfJysrCzs4OX19fXr58iZeXF3379iUkJITY2Fh69uzJwoULiY+Pp1GjRvTr10/x3nnz5hEcHMyNgCCsBizi5ZW9pD2+rng9/tgqZOlJ2I5eg92UnZh3HK10bumLSJDJ0Law42ZoPO7unRk7dixxcXE8fPiQTp06KbU/cuQIHh4eJCcn07hxY65fv058fDzDhg2jS5cuYsFyEYmPj6dLly4sXLgw30Wn3dzcOHHiRCFHln+NGjXi2rVrHD9+nL59+4qfnRJKDHUKKuPi4sLcuXOJi4tj06ZNXLp0CYCUlBTKly/PzZs3cXJywtbWlk2bNvFY15Hlp4KI/ncz0hfPsOw+A2lcOJGbv6LSxM1o6hrkeZ5Ev4NkvYiknNtYks5tobphBldO/PPWuBo2bMiGDRto2LBhrtdMTEw4c+aMYrNSoXBkZmbSqVMnGjVqxM8//5zv92VkZGBpaUlISAjlypUrxAgLJiMjg3HjxnHjxg0OHDiAg4ODukMSCkD0+ASViI6OJigoCGdnZwICAqhXr57iNUNDQxwdHQkICODFixdERkZSr149AqMSyciSoW1VBWlsGAAZz4KQmFqRcH474SsG8uzPiaQEKu/OnvbID33HnGLSKRGBZGgZ0Lx5c6ysrOjWrRthYWGKtpGRkURHR+dZDebWrVtkZmZSrVq1wrgkwv+Ty+WMGzcOMzMzlixZUqD36urq0qpVK06fPl1I0X0YXV1dNm7cyPDhw2nWrBnnzp1Td0hCAYjEJ3w0qVTKoEGDGDZsGE5OTiQnJ+faqdzU1JSkpCTF0JCpqSmJ6VkAaOoaIstIAyA7KQ5pTCiaugZUmrSZcm7jiDuyHGlsziakMmk6mZHB6Nm7/H/7WO75HmLFihWEhYVRpUoVBgwYoDivj48PnTt3zjVdPjExkSFDhjB37lyxq3ohW7JkCbdu3WL79u0fVDGnOD3ne52GhgZffvklmzdvpk+fPqxbt07dIQn5JBKf8FFkMhlDhgxBR0eHVatWAWBkZERiYqJSu8TERIyNjTEyMlJ8bqKXszmILCMVTV19ADQkOqApwbRFfzS0tNGzr4uefV3SntwAID3EH92KTmhItBXtq7m2p3Hjxujp6TF37lwuXbrEy5cvgZzE5+HhoRRLWloa3bp1o2nTpsycObOQrowA4O3tzerVqzl06NAHV8559ZyvuD6V6dSpExcuXGDFihVMmDABqVSq7pCE9xCJT/hgcrmckSNHEh0dzd69exWVLZydnZU290xJSeHRo0c4Oztjbm5OhQoV8Pf3x8nGBF2JJtLnT9Aubw+AtpVD7hO91ltLe+yHvmNjxef61lUwM9B5ren/2kqlUnx9fXFzc1N8LSMjg88++4xKlSqJO/RCdu3aNcaPH8/BgwepWLHiBx+nVq1aZGVlERwcrMLoVKt69epcvnyZsLAwOnXqRGxsrLpDEt5BJD7hg40fP5779+9z6NAh9PX1FV/v0aMHd+/eZe/evaSnp7NgwQJcXFxwcnICYOjQoXh5edGxqiGZseEk+x/HqG7OLD89uzpITCx5eflv5LJs0iPukR52B/2qORNT0h5dVzzfAzCp50bA5VPcunULqVTKwoULadmyJaamply4cAEXFxdMTEyAnETYu3dv9PX12bx5s9h3rRCFhYXRo0cP/vzzz4/ebUNDQ0PtVVzyw9TUlAMHDtCkSROx2L2YE7/5wgcJDQ1l3bp13Lp1CxsbG4yMjDAyMmL79u1YWlqyd+9eZs+ejbm5OVevXmXXrl2K986fPx9HR0caOtcgesdMTJr2RL9qzqxKDS0Jlr2+I+2RH+HL+xJ/dCXlu0xF28KOzJgQNHX0kJha5bTVgC6d3Vi8aBFdunTBysqKhw8fsmPHDuB/yxheuXTpEocPH+bEiROYmZkpYj5//nwRXrnSLzExka5du/L111/j6empkmMW1+d8b9LS0uLHH3/Ey8uLDh06sH//fnWHJORBLGcQ1KoglVteXvFGlpaIebsRAOhra7F7TFNcKpnl2b527dp4e3tTu3ZtVYYsvENWVhaenp7Y29uzZs0ale13+Pz5c2rUqEFMTEyJKRbt5+dHjx49GD16NN99950YYShGxP8JQa3q2Zkx28MJfe33/yhKTK0xqpvzvE6CjNkeTm9NepmZmQwdOlQkvSL21VdfkZWVxcqVK1W6ya+VlRVVqlTh2rVrKjtmYXu12P3o0aP07duXlJQUdYck/D+R+AS1G9zUgdketdDX1uJdfysNa7VCx9IOXYkG2de9ibt64K1tdXR0+PbbbwshWuFtVq1axalTp/j7778LpVf2anPakqRChQqcOXMGY2NjWrRooVS6T1AfkfiEYmFwUwd2j2mKe21rdCWa6EmUfzT1JJroSjRxr23NnrHNOfvHApYvXy5mZhYTPj4+/PDDDxw5cgQzM7NCOUdJmOCSFz09PTZu3MiwYcNo2rSpeKZcDIhnfEKxE5ecgfeNCAIjk0hMl2Kip41TBWN6N1Tegf3Ro0e0bdsWLy8vhg0bpsaIy7bbt2/ToUMHDhw4QPPmzQvtPOnp6VhaWhIeHl5oybWwnThxgsGDB+Pl5cWYMWPUHU6ZJRKfUKIFBgbSvn17li9frlQIWygaUVFRNGnShB9//FGpYk5hcXd3Z9y4cfTo0aPQz1VYgoKC8PT0pGPHjixfvrzETNYpTcRQp1CiOTk5cfz4caZMmSKmjhex1NRUunfvzsiRI4sk6UHJfM73pho1anD16lWePHmCu7u7WOyuBiLxCSVe3bp18fHxYdy4cfj4+Kg7nDJBJpMxbNgwqlevzpw5c4rsvCX1Od+bTE1NOXjwII0bN8bV1ZU7d+6oO6QyRSQ+oVRo2LAhBw4c4PPPPy92lfxLo++++47IyEg2bNig0mUL71O3bl2Sk5N5/PhxkZ2zsGhpabFkyRIWLFhA+/bt+eeff9QdUpkhEp9QajRt2hRvb28GDBggZs4Vok2bNrF7927279+Pnp5ekZ67pJQvK4jBgwfj4+PD5MmT8fLyKrbFuEsTkfiEUqV169bs2LGDXr16cfXqVXWHU+qcPXuW6dOnc+TIESwtLdUSQ2l4zvemxo0bc+3aNQ4fPky/fv3EYvdCJhKfUOp07NiRv/76C09PT27evKnucEqNoKAg+vXrx86dOxUFx9WhY8eOnDlzhuzs95e5K0kqVKjA2bNn0dfXp2XLlmKxeyESiU8olbp06cKaNWvw8PDg7t276g6nxIuLi6Nr166K4svqVKFCBSpVqoSfn59a4ygMenp6bNq0iSFDhojF7oVIJD6h1OrZsyfLli2jU6dOPHjwQN3hlFiZmZn06tWL7t27M3r0aHWHA/xvc9rSSENDg6+++opNmzbRq1cv/vjjD3WHVOqIxCeUagMGDGDRokV07NixVMwELGpyuZwxY8Zgbm7Ojz/+qO5wFErKNkUfw93dnQsXLvDLL78wefJksbO7ConKLUKZsHbtWpYsWYKvry/29vbqDqfEWLx4Md7e3pw7dw5DQ0N1h6OQmpqKtbU1z549w9jYWN3hFKqEhAQGDBhARkYGe/bswcLCQt0hlXiixyeUCePGjWPKlCm0b9+eZ8+eqTucEmHPnj38/vvvHDp0qFglPQADAwOaNGnC2bNn1R1KoTMzM+Pw4cM0atQIV1dX8cxaBUTiE8qML7/8kpEjR9KhQweeP3+u7nCKtatXrzJhwgQOHjyIra2tusPJU2l+zvcmLS0tfvrpJ+bPn0+7du04cODtW3IJ7ydRdwCCUJRmzpxJenq6Ykq8GDbKLTQ0lJ49e7Jx40YaNGig7nDeqlOnTkVWI7S4GDx4MDVq1KBnz57cvXuXWbNmFWnlnNJCPOMTyhy5XM6MGTP4999/OXXqVInd4qYwJCYm0qJFC0aOHMmXX36p7nDeSSaTYWNjg5+fX5l7bvvs2TN69OiBg4MDGzduLHZD0cWdGOoUyhwNDQ2WLFlC8+bN8fDwICkpSd0hFQtZWVn069ePli1bMmXKFHWH816ampp07Nix1M/uzIutrS2+vr7o6urSqlUrwsLC1B1SiSISn1AmaWho8Ouvv1KnTh26detGamqqukNSu6lTpyKTyfjtt99KzPBZWXrO9yY9PT02b97MwIEDadq0KRcuXFB3SCWGGOoUyjSZTMbnn39OVFQUBw8eLPKiy8XFypUrWbt2LZcuXcLU1FTd4eRbREQE9evXJzo6Gi0tLXWHozZHjx5l2LBhLFq0iFGjRqk7nGJPJD6hzMvKymLgwIGkpaWxd+9edHR01B1SkTpy5AijRo3i0qVLVKlSRd3hFFjt2rXZsmULjRo1UncoavXgwQM8PT1xd3dn2bJlSCRi7uLbiKFOocyTSCRs374dLS0tBg4cSFZWlrpDKjK3b99m+PDh7Nu3r0QmPSgbVVzyo2bNmly9epWgoCA6d+5MXFycukMqtkTiEwRAW1ub3bt3k5KSwrBhw0pd5f+8REZG0q1bN1auXEmzZs3UHc4HK8vP+d5kZmbGkSNHqF+/Pk2aNCEgIEDdIRVLYqhTEF6TlpZGly5dqFKlCn/88QeamqXz3jA1NZU2bdrg6enJnDlz1B3OR0lOTqZChQpERUWJaf2v2bJlC19//TUbN26kW7du6g6nWCmdv9WC8IH09fU5ePAgDx48YPLkyaVyN2yZTMbQoUNxcnLiu+++U3c4H83IyIhPPvkEX19fdYdSrAwdOpTDhw8zfvx4Fi1aVCp/lj+USHyC8AYjIyN8fHz477//mDZtWqn7gzF79myio6PZsGFDiVm28D5ubm7iOV8emjRpwtWrV/nnn38YMGCAWLbz/0TiE4Q8mJiYcOzYMU6fPl3ihwJf99dff7Fnzx7279+Prq6uusNRmU6dOonnfG9RsWJFfH190dbWplWrVoSHh6s7JLUTiU8Q3qJcuXKcPHmS/fv34+Xlpe5wPtqZM2f49ttvOXz4MOXLl1d3OCrVsGFDoqKiePr0qbpDKZb09fXZsmUL/fv3p0mTJly6dEndIamVSHyC8A6WlpacPn2aLVu28Msvv6g7nA/24MED+vfvz86dO3FyclJ3OCqnpaVFhw4dxHDnO2hoaPDNN9+wYcMGPvvsMzZu3KjukNRGJD5BeA8bGxtOnz7N6tWrWb16tbrDKbC4uDi6du3KokWLaN++vbrDKTTiOV/+eHh4cO7cOX788UemTJlSptatviKWMwhCPj158oQ2bdowd+5cRo4cqe5w8iUjIwM3NzeaNWvGkiVL1B1OoQoJCcHV1ZWoqKhSuwxFlV68eMGAAQPIzs5m9+7dlCtXTt0hFRnx0yEI+VSlShVOnz7N3Llz2bZtm7rDeS+5XM6YMWOwtLRk8eLF6g6n0Dk4OGBmZsbt27fVHUqJYG5uzuHDh3FxcaFJkybcu3dP3SEVGVHMTRAKoHr16pw4cYIOHTqgp6dH79691R3SWy1atIh79+7h6+tbZnpAr2Z31q9fX92hlAgSiYRffvkFFxcX2rRpU2YWu5eN3wZBUKHatWtz7NgxJk6cyKFDh9QdTp7+/vtv1q1bx8GDBzEwMFB3OEVGPOf7MMOGDePQoUOMGzeOxYsXl7q1q28Sz/gE4QP9999/dOnSha1bt+Lu7q7ucBSuXLmCp6cnJ0+epF69euoOp0glJiZSsWJFnj9/jr6+vrrDKXGePn3KZ599RrVq1fjzzz9L7U2T6PEJwgdq3Lgx+/fvZ/DgwZw9e1bd4QA5Ezx69uzJX3/9VeaSHuQUHqhXrx7nz59XdyglUsWKFTl37hyampq0bt2aiIiIt7aNTc5gre8jvtx9kxGb/+PL3TdZ6/uIuOSMIoz4w4genyB8pDNnztCvXz/++ecfmjdvrrY4Xr58SYsWLRg9ejRTpkxRWxzqtmDBApKSkli6dKm6Qymx5HI5S5cuZcWKFezZs0fp59o/PIHVZx/iGxQDQEaWTPGankQTOdC2piUT2lSjnp1ZEUeePyLxCYIKHD9+nCFDhuDj46OWDVGzsrLo2rUrjo6OrFq1qtTU4PwQly9fZty4cfj7+6s7lBLvyJEjDB8+nCVLljB8+HC2XQnhB59A0rOyeVfm0NAAPYkWsz2cGNzUocjizS+R+ARBRQ4ePMjo0aM5ceJEkQ4zyuVyJk2axOPHjzl06FCZ33k7KysLS0tL7t+/j42NjbrDKfHu37+Pp6cnXb9cwrFofdKksve/6f/pa2sy26NWsUt+4hmfIKiIp6cnq1atonPnzkW6JmrlypX4+vqya9euMp/0IGeKfrt27Th16pS6QykVatWqxfajFzhawKQHkCaV8YNPILcjEgonuA8kEp8gqFCfPn1YunQpnTp1Ijg4uNDPd/jwYX788UcOHz6MqalpoZ+vpOjUqZNY1kBO5Z6RI0dSuXJljI2NqV+/PkePHlW8fvr0aZycnDAwMKBdu3aEhoYqvXfEiBGYmJjQtlFtnl/aq3RsmTSduOO/E75iIGHL+xK1bUau8z9dP5ak6DB+P/uQGzdu0Lp1a4yMjLC2tmbFihVKbS9fvkzz5s0JCwvDyMhI6UNDQ0OltXJF4hMEFRs8eDDz5s2jY8eOhISEFNp5/P39GT58OPv27cPBwaHQzlMSvVrPV9af5GRlZWFnZ4evry8vX77Ey8uLvn37EhISQmxsLD179mThwoXEx8fTqFEj+vXrp3jvvHnzCA4O5kZAEFYDFvHyyl7SHl9XvB5/bBWy9CRsR6/BbspOzDuOVjq39EUkyGRIylXk1M2HuLt3ZuzYscTFxfHw4UM6deqk1P7IkSN4eHhgb29PcnKy4uPOnTtoamrSq1cvlV0X8YxPEArJqlWrWL58Ob6+vlSqVEmlx46MjKRJkyb8/PPP9O3bV6XHLi0cHR05cOAAderUUXcoxYqLiwtz584lLi6OTZs2KbYoSklJoXz58ty8eRMnJydsbW3ZtGkTj3UdWX4qiOh/NyN98QzL7jOQxoUTufkrKk3cjKZu3mv9Ev0OkvUiknJuY0k6t4XqhhlcOfHPW+Nq2LAhGzZsoGHDhkpfnz9/PmfPnuXMmTMquwaixycIhWTSpEmMHz+eDh06EBUVhVwu5/jx48hkBXtO8qaUlBS6devG2LFjRdJ7Bzc3N7E57Ruio6MJCgrC2dmZgIAApUlY0dHRVK5cmYCAAF68eEFkZCT16tUjMCqRjCwZ2lZVkMaGAZDxLAiJqRUJ57cTvmIgz/6cSErgRaVzpT3yQ98xZ4ZzSkQgGVoGNG/eHCsrK7p160ZYWJiibWRkJNHR0TRo0EDpGHK5nC1btjBs2DCVXgeR+AShEE2bNo0hQ4bQoUMHhg8fTufOnT9qcbVMJmPo0KE4Ozsza9YsFUZa+ojnfMqkUimDBg1i2LBhODk5kZycrPRceOrUqTx48IAvv/ySZcuWAWBqakpies62RZq6hsgy0gDITopDGhOKpq4BlSZtppzbOOKOLEcam7O7u0yaTmZkMHr2Lv/fPpZ7vodYsWIFYWFhVKlShQEDBijO7ePjQ+fOnXMtw7lw4QLR0dEqr4krEp8gFLJZs2ahpaXFli1b0NTUZPfu3R91rJiYGNavX1+m1+rlR/v27blw4QLp6enqDkXtZDIZQ4YMQUdHh1WrVgFgZGREYmIicrmcp0+fKkq8RURE8MMPPwAwbtw4TPRyZgrLMlLR1M1poyHRAU0Jpi36o6GljZ59XfTs65L25AYA6SH+6FZ0QkOirWhfzbU9jRs3Rk9Pj7lz53Lp0iVevnwJ5CQ+Dw+PXHFv3ryZXr16YWRkpNLrIeY+C0Ihmzp1Kvfu3UMulyOXy/n7779ZvXq1UuKKTc7A+3oEgVGJJKZnYaInwcnGhD6fVMLCSBeAP//8k71793LlyhV0dXXV9e2UGGZmZtSpU4dLly6V6g1430culzNy5EgiIiLw8vJix44dBAcHc/HiRQIDA9myZQuGhoZKyUVbWxtNTU0+++wzosxN0JVEkfD8Cdrl7XNet3LIfaLXfp7THvuh79hY8bm+dRXMDHRea/q/tlKpFF9fX/766y+lw6WlpbFnzx7279//sZcgF5H4BKGQ9e7dm6CgIM6cOUN2djZxcXGKqdvvLv8UxfJTQbStaYmrYQKzZs3i3LlzWFhYqOtbKXFePecrK4kvKSmJ4OBggoKCCAoKIjg4mOPHjxMfH4+hoSHTp0+nevXq1KhRg1GjRjFt2jR+//13+vbty8iRI3n8+DHm5uZs374dX19fli9fzp/bdrMkNpxk/+NYdPkSAD27OkhMLHl5+W9Mm/Ul49kD0sPuYN5uOABpj65j2rSPIi6Tem4EHPmJW7du4ezszMKFC2nZsiWmpqacOXMGFxcXTExMlL6X/fv3Y25uTrt27VR+ncSsTkEoIi9fvmTv3r18//33TJ06FesWvfJX/gmQZWUwrK4xC4a6FVm8pcGNGzdYsfZPmg366p296ZIkLS2NR48eKRLc6/9NTEykWrVq1KhRg+rVq1OuXDm++eYbdHV1lYobrFu3jkGDBnHq1CkmTZpEaGgojRo1wtnZmR9++AELCwsyMjIYP3483t7eZGloo9e4ByaNeyiOkRkTStzR35DGhCAxscKs9RAMajYnMyaE2AM/YTvqdyCnI+he25p6Sf/h5eVFamoqLVu25Pfff8fOzo5p06ZhY2PDtGnTlL5Pd3d3XF1dWbhwocqvoUh8gqAGOTUP75eK8k/FVUkupiyVSgkJCckzuUVFReHg4KBIbq//19bWtlA2HfYPT6D/H1dIk2a/t+3LK97I0hIxbzcCAH1tLXaPaYpLJbM829euXRtvb29q166typDfSSQ+QShiBfkj8qb3/RERcpSEYsoymYyIiIhcyS0oKIiwsDBsbW1zJbbq1atTuXJltZSmy+/NWsr98+hYOqBd3u69N2uZmZksW7aMb7/9thAifjuR+AQhnzIyMpgwYQKnTp0iPj4eR0dHFi9ezKeffgrklH+aOHEiYWFhNGnShE2bNlG5cmXFexXDRpra6DVSHjaSSdN58e9GUgMvIJdloWNZBZvBS5TO/3T9WKx7f0/XVg0ZU1uTL7/8khs3bmBoaMisWbOUtiK6fPkyX3/9NZcuXcLBwYHo6Gi0tLQAaN68eale31acetNyuZznz5/n6rUFBQXx6NEjzM3N8+y5Va1atVhOYCoJNxT5IZYzCEI+lcTyT68cOnRIUQKqOCY9VdWULG9lzdffL1JKevmpKflw9Wjmbj3F7YiEfNeUfGXFihVUrlwZfX19KlasyKRJkxgwYACNGjXC1NQUZ2dnZsyYga+vL8bGxvTr14+tW7fy/Plznj59ypkzZ1i/fj3Tpk2je/fu1KpVq1gmPYDBTR3YPaYp7rWt0ZVooidRTiF6Ek10JZq417Zm95imxTLpgZjVKQj5ZmhoyLx58xSfd+3alSpVqnD9+nXi4uJwdnamT5+cmWzz5s2jfPnyBAYG4uTkxObNm9m0aROnHqegW94e43ruJN85hX7VT5DGhZMafFWp/JOuTTWlc6c9+k9RBePFlf1Ub9CcQYMG5bTV1aVWrVpK7X18fNiwYUNhXQqVe/2mwt7eHh8fH/r27cudO3cwMjKiZ8+ebNiwgW7dujFnzhz69evHlStXgP/dVISGhjJm3Sn2eY2hvHkl9Kt+AuTcVMhl2diOXoOmnhGZz58onfvVTYXMtAK/HPLj6NyBLF++nN69e5OZmanYhTwlJYWHDx/y888/Y2hoyOeff86FCxcICQlR/D+wtbVFX1+fLl26UL16dcUEk9LEpZIZawc3Ii45A+8bEQRGJpGYLsVETxunCsb0blj8Jw2JxCcIH+j18k9r1qxRKv9kaGiIo6MjAQEBWFtbK8o/+Zx9pij/lBqc84f79fJPKQFn0DIyx7TFQAydWiiOl/bID5PG3YH/L/9U25nmzZvz8OFDmjRpwurVq7G3z1ljlVf5p0GDBiGTyWjQoAFLly4t0v0C86MgNxUJCQn4+flx+vRpOnTooLipyNY24HaK0QffVMjlcGjbH7g2boKxsTErV65UGp6Mi4vD0dGRiIgIunfvTosWLfDx8WHnzp307t27zBUUsDDSZWxrR3WH8UFE4hOED5BX+SdLS0ulNqampiQlJZGcnKz4PDE9pz5hXuWfDGo0p9KkzWQ8DeT5nvnolLdHu7xdnuWf/E/vQ19bCx0dHU6fPk21atWoUKECEomElJQUMjIyaNCgARKJBENDQ2xtbZFIJDx+/BhXV1fatWuHgYEBEolE6UNbW/uDvqbq98XFxREUFEStWrVYt26dUqK+ffs22dnZfPrpp/Tr109xU+F9Padn9jE3Fanh97j8TM7VYcPIyMjA0dGRmTNn0qJFC+zs7IiOjqZRo0Zs2rSJ8PBwxowZw7Nnz7C3t0cikTB06FDmzp1bKDMrBdURiU8QCuhd5Z9el5iYiLGxsaIiRmJi4vvLP2lqKZV/0i5vl2f5p5rN3LhycBtZWVk8f/6c2rVrc/jwYfT19ZkwYQKffvop7dq1IysrC6lUSlZWluJjxIgRNGrUiIYNGyp9/c12b36enp6u9DWpVEp2dnaudvk51rvaSKVSUlNTAXB2dgZAU1OTLVu2KBI75Nx8bNu2DcjZJTww2pSMLNnH3VQkx5OekciFs/9St25dpk+fzurVqxk4cCCgXFPy1RDoiRMnuHPnDgkJCXTq1IlKlSoxerTyM1qheBGJTxAK4FX5p+joaHx8fNDWzklGzs7ObN68WdEuJSWFR48e4ezsjLm5ORUqVMDf3x8nm6oqKf9UzlBPUWD41RCbvb09BgYG3LhxA29v71yVMF4xMjLC1dUVT0/Pj7oWhUEmkzFw4EASExM5cOAAEomEKVOmkJGRwbJly8jKyqJ58+bcu3cPHR0drK2tCQ8Px8nJicTQ/y+Q/JE3FZWd29K4cc71njt3LuXLl+fly5eYmpri4+OjSIKvaltOnz4dMzMzzMzMGDt2LD4+PiLxFXOiPy4IBTB+/Hju37/PoUOHFH/4AHr06MHdu3fZu3cv6enpLFiwABcXF5ycnAAYOnQoXl5edKxqSOb/l38yqtsRUC7/JJdlkx5xj/SwO+hXzdmXLO3RdcXEFvj/8k+XT3Hr1i2kUqlS+acLFy4olX8KCwvj4sWLZGZmkp6eztKlS4mNjaVFi/8N9RUXr99U7N27F21tbTQ0NKhTpw53797F0NAQU1NTTE1N0dDQYOXKlYSGhlKhQgXu3Lmj6E1LP+KmQseqCtoSrdea5q4p6eaWUz2nZs2a6OjoKLUpa8/5SiqR+AQhn0JDQ1m3bh23bt3CxsYGIyMjjIyM2L59O5aWluzdu5fZs2djbm7O1atX2bVrl+K98+fPx9HRkYbONYjeMROTpj0Vsw41tCRY9vqOtEd+hC/vS/zRlZTvMhVtCzsyY0LQ1NFDYmqV01YDunR2Y/GiRXTp0gUrKysePnzIjh07gNzLGJKSkhg/fjzm5uZUrFiRY8eOcfTo0WJZ7zO/NxUtWrTA1dWVMWPGoKGhobipsDOUo5Hw9KNuKszruxHidyZfNxUGBgb069ePn376iaSkJCIiIli/fj1du3YtwqsmfAixgF0QilhpK/+kCqGhoTg4OOSrpuSr4gAODg7A/4oD7PH2Jk2mhUmTXpi4FrymJICuRJNRFg/5demP+aopmZiYyJgxYzhy5AhmZmaMHj2aOXPmiJ5fMScSnyCoQWkq/1ScjNnqx8n70e+sKvLKmzcVr4oprx3cKM/2JfWmQshNDHUKghoMburAbI9a6Gtr8a7OgWGtVuhY2qGvrfXeklo6OjplOukBTGxbDb3XntG9i8TUGqO6/9vtQk+ixYS21fJsm5mZydChQ0XSKyVEj08Q1Oh2RAK/n33ImQcxaADpeewg0K6mJRPaVhOFqfOpONXqFIonkfgEoRgoyeWfiqPSUkxZKBwi8QmCUCqJ3rTwNiLxCYJQqonetPAmkfgEQRCEMkXM6hQEQRDKFJH4BEEQhDJFJD5BEAShTBGJTxAEQShTROITBEEQyhSR+ARBEIQyRSQ+QRAEoUwRiU8QBEEoU0TiEwRBEMoUkfgEQRCEMkUkPkEQBKFMEYlPEARBKFNE4hMEQRDKFJH4BEEQhDJFJD5BEAShTBGJTxAEQShTROITBEEQyhSR+ARBEIQyRSQ+QRAEoUwRiU8QBEEoU0TiEwRBEMqU/wNjFnkgA8E9pQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "goog_graph_1 = nx.DiGraph()\n",
    "\n",
    "for i in range(8):\n",
    "    if (goog_lab['Date'][i][5:6] in first_quarter) and (goog_lab['Date'][i+1][5:6] in first_quarter):\n",
    "        goog_graph_1.add_edges_from( [ ( goog_lab['Date'][i] , goog_lab['Date'][i+1] ) ] )\n",
    "    \n",
    "    elif (goog_lab['Date'][i][5:6] in second_quarter) and (goog_lab['Date'][i+1][5:6] in second_quarter):\n",
    "        goog_graph_1.add_edges_from( [ ( goog_lab['Date'][i] , goog_lab['Date'][i+1] ) ] )\n",
    "        \n",
    "    elif (goog_lab['Date'][i][5:6] in third_quarter) and (goog_lab['Date'][i+1][5:6] in third_quarter):\n",
    "        goog_graph_1.add_edges_from( [ ( goog_lab['Date'][i] , goog_lab['Date'][i+1] ) ] )\n",
    "        \n",
    "    elif (goog_lab['Date'][i][5:6] in fourth_quarter) and (goog_lab['Date'][i+1][5:6] in fourth_quarter):\n",
    "        goog_graph_1.add_edges_from( [ ( goog_lab['Date'][i] , goog_lab['Date'][i+1] ) ] )\n",
    "    \n",
    "    else:\n",
    "        continue\n",
    "    goog_graph_1.add_edges_from( [ ( goog_lab['Date'][i] , str(goog_lab['Label'][i] ) ) ] )\n",
    "\n",
    "goog_graph_1.add_edges_from( [ ( goog_lab['Date'][8] , str(goog_lab['Label'][8]) ) ] )\n",
    "\n",
    "# goog_graph.add_edges_from([(1,2)])\n",
    "pos = nx.kamada_kawai_layout(goog_graph_1)\n",
    "nx.draw(goog_graph_1,pos,with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Open', 'High', 'Low', 'Close', 'Volume', 'KDJ', 'W%R', 'RSI']\n",
      "torch.Size([2168, 8])\n",
      "tensor([[ 1.8832e+02,  1.9026e+02,  1.8495e+02,  ...,  9.9906e-01,\n",
      "         -9.3664e-04,  0.0000e+00],\n",
      "        [ 1.8792e+02,  1.8927e+02,  1.8452e+02,  ...,  5.1310e-02,\n",
      "         -9.4869e-01,  5.6746e+00],\n",
      "        [ 1.8569e+02,  1.8790e+02,  1.8220e+02,  ...,  4.3941e-01,\n",
      "         -5.6059e-01,  1.0596e+00],\n",
      "        ...,\n",
      "        [ 5.2588e+02,  5.3133e+02,  5.2443e+02,  ...,  9.6830e-01,\n",
      "         -3.1702e-02, -4.2313e+00],\n",
      "        [ 5.2928e+02,  5.3255e+02,  5.2711e+02,  ...,  5.8330e-02,\n",
      "         -9.4167e-01,  4.6797e+00],\n",
      "        [ 5.2520e+02,  5.2825e+02,  5.2425e+02,  ...,  8.1841e-01,\n",
      "         -1.8159e-01,  9.1052e-01]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Prepping the feature tensor matrix\n",
    "'''\n",
    "# 2707 x 8 --> feature tensor size\n",
    "import torch\n",
    "'''\n",
    "\n",
    "[\n",
    "    ['Open', 'High', 'Low', 'Close', 'Volume', 'KDJ', 'W%R', 'RSI'],\n",
    "    [],\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "    []\n",
    "]\n",
    "\n",
    "'''\n",
    "print(list(goog_lab.columns[1:9]))\n",
    "feature_set_=[]\n",
    "feature_set_.append(list(goog_lab.iloc[0,1:9])) # for label 0\n",
    "feature_set_.append(list(goog_lab.iloc[2,1:9])) # for label 1\n",
    "feature_set_.append(list(goog_lab.iloc[3,1:9])) # for label 2\n",
    "# num_of_rows\n",
    "for i in range(num_of_rows):\n",
    "    feature_set_.append(list(goog_lab.iloc[i,1:9]))\n",
    "# feature_set = torch.tensor(feature_set,requires_grad=True)\n",
    "feature_set = torch.tensor(feature_set_,dtype=torch.double)\n",
    "print(feature_set.size())\n",
    "feature_set = feature_set.double()\n",
    "print(feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2168])\n",
      "tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        ...,\n",
      "        [0],\n",
      "        [1],\n",
      "        [0]])\n"
     ]
    }
   ],
   "source": [
    "# lis = [0,1,2]\n",
    "goog_lab_tensor_ = list(goog_lab['Label'])\n",
    "goog_lab_tensor = list(goog_lab['Label'])\n",
    "\n",
    "goog_lab_tensor.insert(0,0)\n",
    "goog_lab_tensor.insert(1,1)\n",
    "goog_lab_tensor.insert(2,2)\n",
    "\n",
    "goog_lab_tensor = torch.tensor(goog_lab_tensor)\n",
    "print(goog_lab_tensor.size())\n",
    "print(goog_lab_tensor.resize(num_of_rows+3,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2168\n",
      "[[1, 0, 0], [0, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "goog_lab_tensor__ = list(goog_lab['Label'])\n",
    "goog_lab_tensor__.insert(0,0)\n",
    "goog_lab_tensor__.insert(1,1)\n",
    "goog_lab_tensor__.insert(2,2)\n",
    "print(len(goog_lab_tensor__))\n",
    "\n",
    "goog_lab_tensor_prob = []\n",
    "\n",
    "for i in goog_lab_tensor__:\n",
    "    if(i==0):\n",
    "        goog_lab_tensor_prob.append([1,0,0])\n",
    "    if(i==1):\n",
    "        goog_lab_tensor_prob.append([0,1,0])\n",
    "    if(i==2):\n",
    "        goog_lab_tensor_prob.append([0,0,1])\n",
    "\n",
    "print(goog_lab_tensor_prob[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([[1, 0, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 0, 1],\n",
      "        ...,\n",
      "        [1, 0, 0],\n",
      "        [0, 1, 0],\n",
      "        [1, 0, 0]])\n",
      "torch.Size([2168, 3])\n"
     ]
    }
   ],
   "source": [
    "goog_lab_tensor_prob = torch.tensor(goog_lab_tensor_prob)\n",
    "print(type(goog_lab_tensor_prob))\n",
    "print(goog_lab_tensor_prob)\n",
    "print(goog_lab_tensor_prob.size())\n",
    "\n",
    "# len(goog_lab_tensor_)\n",
    "\n",
    "# goog_lab_tensor_ = torch.tensor(goog_lab_tensor_)\n",
    "# print(goog_lab_tensor_.size())\n",
    "# print(goog_lab_tensor_.resize(1,num_of_rows+3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"Tensor\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\PYTORC~1\\AppData\\Local\\Temp/ipykernel_6532/779565954.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m# train_dataloader = DataLoader(feature_set, batch_size=60,shuffle=False)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mip\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;31m# print(i for i in enumerate(train_dataloader))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\software installers\\python_installation\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\software installers\\python_installation\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\software installers\\python_installation\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\software installers\\python_installation\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\PYTORC~1\\AppData\\Local\\Temp/ipykernel_6532/779565954.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m# Load data and get label\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mID\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mID\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"Tensor\") to str"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "\n",
    "# class Dataset(torch.utils.data.Dataset):\n",
    "#   'Characterizes a dataset for PyTorch'\n",
    "#   def __init__(self, list_IDs, labels):\n",
    "#         'Initialization'\n",
    "#         self.labels = labels\n",
    "#         self.list_IDs = list_IDs\n",
    "\n",
    "#   def __len__(self):\n",
    "#         'Denotes the total number of samples'\n",
    "#         return len(self.list_IDs)\n",
    "\n",
    "#   def __getitem__(self, index):\n",
    "#         'Generates one sample of data'\n",
    "#         # Select sample\n",
    "#         ID = self.list_IDs[index]\n",
    "\n",
    "#         # Load data and get label\n",
    "#         X = torch.load('data/' + ID + '.pt')\n",
    "#         y = self.labels[ID]\n",
    "\n",
    "#         return X, y\n",
    "\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# training_set = Dataset(feature_set, goog_lab_tensor_prob)\n",
    "# train_dataloader = torch.utils.data.DataLoader(training_set, batch_size=64, shuffle=False)\n",
    "\n",
    "# # train_dataloader = DataLoader(feature_set, batch_size=60,shuffle=False)\n",
    "# ip,op = enumerate(train_dataloader)\n",
    "\n",
    "# # print(i for i in enumerate(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2168, 3])\n",
      "torch.Size([6504])\n"
     ]
    }
   ],
   "source": [
    "# print(goog_lab_tensor_prob.size())\n",
    "# goog_lab_tensor_prob = goog_lab_tensor_prob.resize(6504)\n",
    "# print(goog_lab_tensor_prob.size())\n",
    "# goog_lab_tensor_prob = goog_lab_tensor_prob.resize(2168*3)\n",
    "# print(goog_lab_tensor_prob.size())\n",
    "# goog_lab_tensor_prob = goog_lab_tensor_prob.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2168"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(goog_lab_tensor_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "2168\n"
     ]
    }
   ],
   "source": [
    "# 8 -> 6 -> 3\n",
    "\n",
    "'''Creation of GCN starts'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn.pytorch import GraphConv\n",
    "import dgl\n",
    "\n",
    "# print('gpu') if torch.cuda.is_available else print('cpu')\n",
    "goog_graph_dgl = dgl.from_networkx(goog_graph)\n",
    "# print(goog_graph_dgl.num_edges())\n",
    "goog_graph_dgl.ndata['feature'] = feature_set\n",
    "# print(goog_graph_dgl)\n",
    "goog_graph_dgl.ndata['feature'][[0,1]]\n",
    "print(goog_graph_dgl.ndata['feature'].shape[1])\n",
    "print(goog_graph_dgl.ndata['feature'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (conv1): GraphConv(in=8, out=6, normalization=both, activation=None)\n",
       "  (conv2): GraphConv(in=6, out=3, normalization=both, activation=None)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes):\n",
    "        # super(GCN, self).__init__()\n",
    "        super().__init__()\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size, weight=True, bias=True)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes, weight=True, bias=True)\n",
    "\n",
    "    def forward(self, goog_graph_dgl, inputs):\n",
    "        h = self.conv1(goog_graph_dgl, inputs)\n",
    "        h = torch.relu(h)\n",
    "        h = self.conv2(goog_graph_dgl, h)\n",
    "        return h\n",
    "net = GCN(goog_graph_dgl.ndata['feature'].shape[1], 6, 3)\n",
    "net.double()\n",
    "\n",
    "# norm='both', weight=True, bias=True\n",
    "# 0 , 1 , 2\n",
    "\n",
    "# inputs = embed.weight\n",
    "# # print(inputs.size())\n",
    "# labeled_nodes = torch.tensor([0, 33])  # only the instructor and the president nodes are labeled\n",
    "# labels = torch.tensor([0, 1])  # their labels are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 0 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 1 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 2 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 3 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 4 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 5 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 6 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 7 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 8 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 9 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 10 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 11 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 12 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 13 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 14 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 15 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 16 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 17 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 18 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 19 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 20 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 21 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 22 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 23 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 24 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 25 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 26 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 27 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 28 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 29 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 30 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 31 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 32 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 33 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 34 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 35 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 36 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 37 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 38 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 39 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 40 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 41 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 42 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 43 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 44 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 45 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 46 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 47 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 48 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 49 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 50 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 51 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 52 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 53 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 54 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 55 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 56 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 57 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 58 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 59 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 60 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 61 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 62 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 63 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 64 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 65 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 66 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 67 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 68 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 69 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 70 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 71 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 72 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 73 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 74 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 75 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 76 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 77 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 78 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 79 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 80 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 81 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 82 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 83 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 84 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 85 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 86 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 87 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 88 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 89 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 90 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 91 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 92 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 93 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 94 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 95 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 96 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 97 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 98 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 99 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 100 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 101 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 102 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 103 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 104 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 105 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 106 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 107 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 108 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 109 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 110 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 111 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 112 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 113 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 114 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 115 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 116 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 117 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 118 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 119 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 120 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 121 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 122 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 123 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 124 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 125 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 126 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 127 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 128 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 129 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 130 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 131 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 132 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 133 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 134 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 135 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 136 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 137 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 138 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 139 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 140 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 141 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 142 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 143 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 144 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 145 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 146 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 147 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 148 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 149 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 150 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 151 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 152 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 153 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 154 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 155 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 156 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 157 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 158 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 159 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 160 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 161 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 162 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 163 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 164 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 165 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 166 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 167 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 168 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 169 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 170 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 171 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 172 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 173 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 174 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 175 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 176 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 177 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 178 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 179 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 180 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 181 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 182 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 183 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 184 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 185 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 186 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 187 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 188 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 189 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 190 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 191 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 192 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 193 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 194 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 195 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 196 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 197 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 198 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 199 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 200 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 201 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 202 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 203 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 204 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 205 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 206 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 207 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 208 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 209 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 210 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 211 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 212 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 213 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 214 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 215 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 216 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 217 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 218 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 219 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 220 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 221 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 222 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 223 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 224 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 225 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 226 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 227 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 228 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 229 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 230 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 231 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 232 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 233 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 234 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 235 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 236 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 237 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 238 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 239 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 240 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 241 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 242 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 243 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 244 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 245 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 246 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 247 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 248 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 249 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 250 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 251 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 252 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 253 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 254 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 255 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 256 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 257 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 258 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 259 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 260 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 261 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 262 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 263 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 264 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 265 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 266 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 267 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 268 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 269 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 270 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 271 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 272 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 273 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 274 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 275 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 276 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 277 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 278 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 279 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 280 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 281 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 282 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 283 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 284 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 285 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 286 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 287 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 288 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 289 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 290 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 291 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 292 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 293 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 294 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 295 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 296 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 297 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 298 | Loss: 0.4560\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward>)\n",
      "Epoch 299 | Loss: 0.4560\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "criterion = nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(itertools.chain(net.parameters(), feature_set), lr=0.01)\n",
    "all_logits = []\n",
    "logp_ = 0\n",
    "logits_ = 0\n",
    "\n",
    "for epoch in range(300):\n",
    "    logits = net(goog_graph_dgl, feature_set)\n",
    "    all_logits.append(logits.detach())\n",
    "    logp = F.softmax(logits, dim=1)\n",
    "#     print(logp.size())\n",
    "    print(logp)\n",
    "#     print(goog_lab_tensor_prob.size())\n",
    "    loss_ = criterion(logp.float(), goog_lab_tensor_prob.float())\n",
    "#     acc = (logp == goog_lab_tensor).float().mean()\n",
    "\n",
    "    loss_.type(torch.LongTensor)\n",
    "    print(loss_)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss_.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('Epoch %d | Loss: %.4f' % (epoch, loss_.item()))\n",
    "#     print(f'Epoch {epoch} | Loss : {loss.item()} | Acc : {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>--End of implementation--</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2165\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "542\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<generator object DataFrame.iterrows at 0x000002759F392EB0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "goog_lab = pd.read_csv('goog_labeled.csv')\n",
    "goog_lab, goog_lab_test = train_test_split(goog_lab, test_size=0.2, train_size=0.8, shuffle=False)\n",
    "print(len(goog_lab))\n",
    "print(type(goog_lab))\n",
    "print(len(goog_lab_test))\n",
    "goog_lab.iterrows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>KDJ</th>\n",
       "      <th>W%R</th>\n",
       "      <th>RSI</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006/5/25</td>\n",
       "      <td>188.315186</td>\n",
       "      <td>190.262527</td>\n",
       "      <td>184.952072</td>\n",
       "      <td>190.257553</td>\n",
       "      <td>16495700</td>\n",
       "      <td>0.999063</td>\n",
       "      <td>-0.000937</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2006/5/26</td>\n",
       "      <td>191.032516</td>\n",
       "      <td>191.693222</td>\n",
       "      <td>188.787125</td>\n",
       "      <td>189.442856</td>\n",
       "      <td>7381600</td>\n",
       "      <td>0.225640</td>\n",
       "      <td>-0.774360</td>\n",
       "      <td>1.814697</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2006/5/30</td>\n",
       "      <td>187.917770</td>\n",
       "      <td>189.268982</td>\n",
       "      <td>184.524841</td>\n",
       "      <td>184.768265</td>\n",
       "      <td>8688100</td>\n",
       "      <td>0.051310</td>\n",
       "      <td>-0.948690</td>\n",
       "      <td>5.674591</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2006/5/31</td>\n",
       "      <td>185.692245</td>\n",
       "      <td>187.902878</td>\n",
       "      <td>182.204926</td>\n",
       "      <td>184.708649</td>\n",
       "      <td>16066300</td>\n",
       "      <td>0.439408</td>\n",
       "      <td>-0.560592</td>\n",
       "      <td>1.059616</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2006/6/1</td>\n",
       "      <td>185.563095</td>\n",
       "      <td>190.257553</td>\n",
       "      <td>184.599365</td>\n",
       "      <td>190.073761</td>\n",
       "      <td>12637600</td>\n",
       "      <td>0.967518</td>\n",
       "      <td>-0.032482</td>\n",
       "      <td>-4.365112</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2160</th>\n",
       "      <td>2014/12/23</td>\n",
       "      <td>524.118103</td>\n",
       "      <td>531.636780</td>\n",
       "      <td>523.411987</td>\n",
       "      <td>527.688477</td>\n",
       "      <td>2203600</td>\n",
       "      <td>0.519951</td>\n",
       "      <td>-0.480049</td>\n",
       "      <td>-4.688721</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2161</th>\n",
       "      <td>2014/12/24</td>\n",
       "      <td>527.608948</td>\n",
       "      <td>528.851074</td>\n",
       "      <td>524.138000</td>\n",
       "      <td>525.878418</td>\n",
       "      <td>707800</td>\n",
       "      <td>0.369274</td>\n",
       "      <td>-0.630726</td>\n",
       "      <td>2.810059</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2162</th>\n",
       "      <td>2014/12/26</td>\n",
       "      <td>525.878418</td>\n",
       "      <td>531.328491</td>\n",
       "      <td>524.426392</td>\n",
       "      <td>531.109680</td>\n",
       "      <td>1043400</td>\n",
       "      <td>0.968298</td>\n",
       "      <td>-0.031702</td>\n",
       "      <td>-4.231262</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2163</th>\n",
       "      <td>2014/12/29</td>\n",
       "      <td>529.279724</td>\n",
       "      <td>532.551758</td>\n",
       "      <td>527.112671</td>\n",
       "      <td>527.429932</td>\n",
       "      <td>2284800</td>\n",
       "      <td>0.058330</td>\n",
       "      <td>-0.941670</td>\n",
       "      <td>4.679748</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2164</th>\n",
       "      <td>2014/12/30</td>\n",
       "      <td>525.202148</td>\n",
       "      <td>528.245422</td>\n",
       "      <td>524.247375</td>\n",
       "      <td>527.519409</td>\n",
       "      <td>878600</td>\n",
       "      <td>0.818408</td>\n",
       "      <td>-0.181592</td>\n",
       "      <td>0.910523</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2165 rows  10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date        Open        High         Low       Close    Volume  \\\n",
       "0      2006/5/25  188.315186  190.262527  184.952072  190.257553  16495700   \n",
       "1      2006/5/26  191.032516  191.693222  188.787125  189.442856   7381600   \n",
       "2      2006/5/30  187.917770  189.268982  184.524841  184.768265   8688100   \n",
       "3      2006/5/31  185.692245  187.902878  182.204926  184.708649  16066300   \n",
       "4       2006/6/1  185.563095  190.257553  184.599365  190.073761  12637600   \n",
       "...          ...         ...         ...         ...         ...       ...   \n",
       "2160  2014/12/23  524.118103  531.636780  523.411987  527.688477   2203600   \n",
       "2161  2014/12/24  527.608948  528.851074  524.138000  525.878418    707800   \n",
       "2162  2014/12/26  525.878418  531.328491  524.426392  531.109680   1043400   \n",
       "2163  2014/12/29  529.279724  532.551758  527.112671  527.429932   2284800   \n",
       "2164  2014/12/30  525.202148  528.245422  524.247375  527.519409    878600   \n",
       "\n",
       "           KDJ       W%R       RSI  Label  \n",
       "0     0.999063 -0.000937  0.000000      0  \n",
       "1     0.225640 -0.774360  1.814697      0  \n",
       "2     0.051310 -0.948690  5.674591      1  \n",
       "3     0.439408 -0.560592  1.059616      2  \n",
       "4     0.967518 -0.032482 -4.365112      0  \n",
       "...        ...       ...       ...    ...  \n",
       "2160  0.519951 -0.480049 -4.688721      1  \n",
       "2161  0.369274 -0.630726  2.810059      2  \n",
       "2162  0.968298 -0.031702 -4.231262      0  \n",
       "2163  0.058330 -0.941670  4.679748      1  \n",
       "2164  0.818408 -0.181592  0.910523      0  \n",
       "\n",
       "[2165 rows x 10 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goog_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014']\n"
     ]
    }
   ],
   "source": [
    "# [Jan,Feb,March]\n",
    "# (1,2,3)\n",
    "\n",
    "# [April,May,Jun]\n",
    "# (4,5,6)\n",
    "\n",
    "# [Jul,Aug,Sep]\n",
    "# (7,8,9)\n",
    "\n",
    "# [Oct,Nov,Dec]\n",
    "# (10,11,12)\n",
    "\n",
    "# Connect graphs based on these relationships [Quarter-wise]\n",
    "first_quarter = ('1','2','3')\n",
    "second_quarter = ('4','5','6')\n",
    "third_quarter = ('7','8','9')\n",
    "fourth_quarter = ('10','11','12')\n",
    "\n",
    "list_of_years = []\n",
    "num_of_rows = len(goog_lab.index)\n",
    "# pd.unique(goog_lab['Date'])\n",
    "# for i in range(num_of_rows):\n",
    "#     if goog_lab['Date'][i][:4] not in list_of_years:\n",
    "#         list_of_years.append(goog_lab['Date'][i][:4])\n",
    "#     else:\n",
    "#         continue\n",
    "for _ , row in goog_lab.iterrows():\n",
    "    if row['Date'][:4] not in list_of_years:\n",
    "        list_of_years.append(row['Date'][:4])\n",
    "    else:\n",
    "        continue\n",
    "print(list_of_years)\n",
    "# goog_lab['Date'][0][:4]=='2006'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "goog_graph = nx.Graph()\n",
    "\n",
    "# goog_graph.add_nodes_from(['0','1','2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_of_rows-1):\n",
    "    if (goog_lab.iloc[i]['Date'][5:6] in first_quarter) and (goog_lab.iloc[i+1]['Date'][5:6] in first_quarter):\n",
    "        goog_graph.add_edges_from( [ ( goog_lab.iloc[i]['Date'] , goog_lab.iloc[i+1]['Date'] ) ] )\n",
    "    \n",
    "    elif (goog_lab.iloc[i]['Date'][5:6] in second_quarter) and (goog_lab.iloc[i+1]['Date'][5:6] in second_quarter):\n",
    "        goog_graph.add_edges_from( [ ( goog_lab.iloc[i]['Date'] , goog_lab.iloc[i+1]['Date'] ) ] )\n",
    "        \n",
    "    elif (goog_lab.iloc[i]['Date'][5:6] in third_quarter) and (goog_lab.iloc[i+1]['Date'][5:6] in third_quarter):\n",
    "        goog_graph.add_edges_from( [ ( goog_lab.iloc[i]['Date'] , goog_lab.iloc[i+1]['Date'] ) ] )\n",
    "        \n",
    "    elif (goog_lab.iloc[i]['Date'][5:6] in fourth_quarter) and (goog_lab.iloc[i+1]['Date'][5:6] in fourth_quarter):\n",
    "        goog_graph.add_edges_from( [ ( goog_lab.iloc[i]['Date'] , goog_lab.iloc[i+1]['Date'] ) ] )\n",
    "    \n",
    "    else:\n",
    "        continue\n",
    "#     goog_graph.add_edges_from( [ ( goog_lab.iloc[i]['Date'] , str(goog_lab.iloc[i]['Label']) ) ] )\n",
    "\n",
    "# goog_graph.add_edges_from( [ ( goog_lab.iloc[num_of_rows-1]['Date'] , str(goog_lab.iloc[num_of_rows-1]['Label']) ) ] )\n",
    "\n",
    "# goog_graph.add_edges_from([(1,2)])\n",
    "\n",
    "# pos = nx.kamada_kawai_layout(goog_graph)\n",
    "# nx.draw(goog_graph,pos,with_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Open', 'High', 'Low', 'Close', 'Volume', 'KDJ', 'W%R', 'RSI', 'Label']\n",
      "torch.Size([2165, 9])\n",
      "tensor([[ 1.8832e+02,  1.9026e+02,  1.8495e+02,  ..., -9.3664e-04,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 1.9103e+02,  1.9169e+02,  1.8879e+02,  ..., -7.7436e-01,\n",
      "          1.8147e+00,  0.0000e+00],\n",
      "        [ 1.8792e+02,  1.8927e+02,  1.8452e+02,  ..., -9.4869e-01,\n",
      "          5.6746e+00,  1.0000e+00],\n",
      "        ...,\n",
      "        [ 5.2588e+02,  5.3133e+02,  5.2443e+02,  ..., -3.1702e-02,\n",
      "         -4.2313e+00,  0.0000e+00],\n",
      "        [ 5.2928e+02,  5.3255e+02,  5.2711e+02,  ..., -9.4167e-01,\n",
      "          4.6797e+00,  1.0000e+00],\n",
      "        [ 5.2520e+02,  5.2825e+02,  5.2425e+02,  ..., -1.8159e-01,\n",
      "          9.1052e-01,  0.0000e+00]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Prepping the feature tensor matrix\n",
    "'''\n",
    "# 2707 x 8 --> feature tensor size\n",
    "import torch\n",
    "'''\n",
    "\n",
    "[\n",
    "    ['Open', 'High', 'Low', 'Close', 'Volume', 'KDJ', 'W%R', 'RSI', 'Label'],\n",
    "    [],\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "    []\n",
    "]\n",
    "\n",
    "'''\n",
    "print(list(goog_lab.columns[1:10]))\n",
    "feature_set_=[]\n",
    "# feature_set_.append(list(goog_lab.iloc[0,1:9])) # for label 0\n",
    "# feature_set_.append(list(goog_lab.iloc[2,1:9])) # for label 1\n",
    "# feature_set_.append(list(goog_lab.iloc[3,1:9])) # for label 2\n",
    "# num_of_rows\n",
    "for i in range(num_of_rows):\n",
    "    feature_set_.append(list(goog_lab.iloc[i,1:10]))\n",
    "# feature_set = torch.tensor(feature_set,requires_grad=True)\n",
    "feature_set = torch.tensor(feature_set_,dtype=torch.double)\n",
    "print(feature_set.size())\n",
    "feature_set = feature_set.double()\n",
    "print(feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2165\n",
      "Data(edge_index=[2, 4276])\n"
     ]
    }
   ],
   "source": [
    "print(len(goog_graph.nodes()))\n",
    "import torch_geometric\n",
    "\n",
    "goog_graph_geo = torch_geometric.utils.from_networkx(goog_graph)\n",
    "print(goog_graph_geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goog_graph_geo.x=feature_set\n",
    "goog_graph_geo.is_directed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2165])\n",
      "tensor([[0],\n",
      "        [0],\n",
      "        [1],\n",
      "        ...,\n",
      "        [0],\n",
      "        [1],\n",
      "        [0]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\software installers\\python_installation\\lib\\site-packages\\torch\\_tensor.py:490: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    }
   ],
   "source": [
    "# lis = [0,1,2]\n",
    "goog_lab_tensor_ = list(goog_lab['Label'])\n",
    "goog_lab_tensor = list(goog_lab['Label'])\n",
    "\n",
    "# goog_lab_tensor.insert(0,0)\n",
    "# goog_lab_tensor.insert(1,1)\n",
    "# goog_lab_tensor.insert(2,2)\n",
    "\n",
    "goog_lab_tensor = torch.tensor(goog_lab_tensor)\n",
    "print(goog_lab_tensor.size())\n",
    "print(goog_lab_tensor.resize(num_of_rows,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2165\n",
      "[[1, 0, 0], [1, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "goog_lab_tensor__ = list(goog_lab['Label'])\n",
    "# goog_lab_tensor__.insert(0,0)\n",
    "# goog_lab_tensor__.insert(1,1)\n",
    "# goog_lab_tensor__.insert(2,2)\n",
    "print(len(goog_lab_tensor__))\n",
    "\n",
    "goog_lab_tensor_prob = []\n",
    "\n",
    "for i in goog_lab_tensor__:\n",
    "    if(i==0):\n",
    "        goog_lab_tensor_prob.append([1,0,0])\n",
    "    if(i==1):\n",
    "        goog_lab_tensor_prob.append([0,1,0])\n",
    "    if(i==2):\n",
    "        goog_lab_tensor_prob.append([0,0,1])\n",
    "\n",
    "print(goog_lab_tensor_prob[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([[1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [0, 1, 0],\n",
      "        ...,\n",
      "        [1, 0, 0],\n",
      "        [0, 1, 0],\n",
      "        [1, 0, 0]])\n",
      "torch.Size([2165, 3])\n"
     ]
    }
   ],
   "source": [
    "goog_lab_tensor_prob = torch.tensor(goog_lab_tensor_prob)\n",
    "print(type(goog_lab_tensor_prob))\n",
    "print(goog_lab_tensor_prob)\n",
    "print(goog_lab_tensor_prob.size())\n",
    "\n",
    "# len(goog_lab_tensor_)\n",
    "\n",
    "# goog_lab_tensor_ = torch.tensor(goog_lab_tensor_)\n",
    "# print(goog_lab_tensor_.size())\n",
    "# print(goog_lab_tensor_.resize(1,num_of_rows+3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, list_IDs, labels):\n",
    "        'Initialization'\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        ID = self.list_IDs[index]\n",
    "\n",
    "        # Load data and get label\n",
    "        X = torch.load('data/' + ID + '.pt')\n",
    "        y = self.labels[ID]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "training_set = Dataset(feature_set, goog_lab_tensor_prob)\n",
    "train_dataloader = torch.utils.data.DataLoader(training_set, batch_size=64, shuffle=False)\n",
    "\n",
    "# train_dataloader = DataLoader(feature_set, batch_size=60,shuffle=False)\n",
    "ip,op = enumerate(train_dataloader)\n",
    "\n",
    "# print(i for i in enumerate(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DGL GCN\n",
    "'''\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes):\n",
    "        # super(GCN, self).__init__()\n",
    "        super().__init__()\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size, weight=True, bias=True)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes, weight=True, bias=True)\n",
    "\n",
    "    def forward(self, goog_graph_dgl, inputs):\n",
    "        h = self.conv1(goog_graph_dgl, inputs)\n",
    "        h = torch.relu(h)\n",
    "        h = self.conv2(goog_graph_dgl, h)\n",
    "        return h\n",
    "net = GCN(goog_graph_dgl.ndata['feature'].shape[1], 6, 3)\n",
    "net.double()\n",
    "\n",
    "# norm='both', weight=True, bias=True\n",
    "# 0 , 1 , 2\n",
    "\n",
    "# inputs = embed.weight\n",
    "# # print(inputs.size())\n",
    "# labeled_nodes = torch.tensor([0, 33])  # only the instructor and the president nodes are labeled\n",
    "# labels = torch.tensor([0, 1])  # their labels are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PyT Geometric GCN\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(9, 7)\n",
    "        self.conv2 = GCNConv(7, 5)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net()\n",
    "model.double()\n",
    "# data = dataset[0].to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "a=0\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(goog_graph_geo)\n",
    "    a=out\n",
    "    print(out)\n",
    "#     loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "#     loss.backward()\n",
    "#     optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "_, pred = model(data).max(dim=1)\n",
    "correct = int(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "acc = correct / int(data.test_mask.sum())\n",
    "print('Accuracy: {:.4f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "class GraphDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self,graphData):\n",
    "        self.graphDataFame = graphData\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphDataFame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        nodeId = self.graphDataFame.iloc[idx]['Date']\n",
    "        nodeFeature = np.array(self.graphDataFame.iloc[idx].drop(['Date','Label'])).astype(np.float32)\n",
    "        nodeLabel = np.array(self.gr\n",
    "nodeLabel = np.array(self.graphDataFame.iloc[idx]['Label'])      \n",
    "\n",
    "        return {'nodeId': nodeId,\n",
    "            'nodeFeature': torch.tensor(nodeFeature.reshape([-1,1])),\n",
    "            'nodeLabel': torch.tensor(nodeLabel, dtype = torch.long)}\n",
    "                             \n",
    "train_test_split ---> datsetobject ---> dataloader\n",
    "total 100 datapint\n",
    "with 10 batchsize\n",
    "                             \n",
    "https://pytorch-geometric-temporal.readthedocs.io/en/latest/_modules/torch_geometric_temporal/nn/attention/stgcn.html\n",
    "https://pytorch-geometric-temporal.readthedocs.io/en/latest/_modules/index.html"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Anomaly detection in financial services using knowledge graphs and machine learning",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
